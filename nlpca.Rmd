---
output:
  word_document: default
  html_document: default
---
```{r}
setwd(dirname("G:/OneDrive/HCMUS_CaoHoc/Luận văn đề tài PCA/code"))
getwd()

```

## Section 1: Configuration và Setup

```{r}
# PHẦN 2: TRIỂN KHAI THUẬT TOÁN GIFI::PRINCIPALS
# Mục 1: Thiết lập cấu hình và mô-đun hóa

# Thiết lập chính cho phân tích NLPCA
NLPCA_CONFIG <- list(
  # Lựa chọn phương pháp - thiết kế dạng mô-đun
  methods = list(
    gifi = TRUE,       # Đang triển khai phương pháp này
    homals = FALSE     # homal
  ),
  
  # Tham số phân tích
  parameters = list(
    ndim = 10,                    # Số lượng thành phần cần trích xuất
    max_iterations = 100,         # Số vòng lặp tối đa cho thuật toán hội tụ
    convergence_tolerance = 1e-6, # Ngưỡng hội tụ thuật toán
    verbose = TRUE,               # Hiển thị chi tiết tiến trình
    seed = 123                    # Giá trị khởi tạo ngẫu nhiên (để tái lập kết quả)
  ),
  
  # Thiết lập trực quan hóa
  visualization = list(
    create_plots = TRUE,        # Có thực hiện vẽ biểu đồ không
    save_plots = FALSE,         # Có lưu file biểu đồ không
    plot_width = 10,            # Chiều rộng biểu đồ (inch)
    plot_height = 8             # Chiều cao biểu đồ (inch)
  ),
  
  # Thiết lập đầu ra
  output = list(
    export_matrices = TRUE,         # Xuất các ma trận kết quả
    detailed_analysis = TRUE,       # Báo cáo chi tiết phân tích
    prepare_for_variable_selection = TRUE  # Chuẩn bị đầu ra phục vụ giai đoạn chọn biến
  )
)


cat("Hoàn tất các thiết lập để triển khai thuật toán NLPCA\n")

```

## Section 2: Nạp gói thư viện và chuẩn bị môi trường phân tích

```{r}
# Mục 2: Nạp gói thư viện và chuẩn bị môi trường phân tích

cat("2.2 CHUẨN BỊ MÔI TRƯỜNG VÀ DỮ LIỆU\n")
cat("\n\n")
cat("- Thiết lập môi trường phân tích R phù hợp để đảm bảo mọi bước tính toán có thể thực thi ổn định.\n")
cat("- Kiểm tra và cài đặt các thư viện cần thiết phục vụ thuật toán phân tích và trực quan hoá kết quả\n\n")

cat("2.2.1 Cài đặt các thư viện cần thiết:\n")

# Nhóm gói thư viện cốt lõi cho triển khai giải thuật Gifi
gifi_packages <- c("Gifi", "MASS", "psych")
# Các thư viện để trực quan hoá dữ liệu
viz_packages <- c("ggplot2", "corrplot", "RColorBrewer", "gridExtra", "knitr")

# Các gói để trực quan hoá dữ liệu
required_packages <- gifi_packages
if(NLPCA_CONFIG$visualization$create_plots) {
  required_packages <- c(required_packages, viz_packages)
}

# Kiểm tra cài đặt các thư viện cần thiết
for(pkg in required_packages) {
  if (!requireNamespace(pkg, quietly = TRUE)) {
    install.packages(pkg, quiet = TRUE)
  }
  library(pkg, character.only = TRUE, quietly = TRUE)
}

cat("✓ Đã cài đặt", length(required_packages), "thư viện cho triển khai giải thuật Gifi\n\n")
```

## Section 3: Load dữ liệu

```{r}
# Section 3: Đọc dữ liệu marketing_data.csv
cat("2.2.2 Đọc dữ liệu từ file csv:\n")
df_raw <- read.csv("marketing_data.csv", stringsAsFactors = FALSE)

cat("Đã đọc file dữ liệu marketing_data thành công\n")
cat("Kích thước bộ dữ liệu:", dim(df_raw), "\n")
cat("Một số tên biến mẫu:", paste(names(df_raw)[1:5], collapse = ", "), "...\n\n")

# Hiển thị cấu trúc tổng quát của bộ dữ liệu để kiểm tra nhanh loại biến và format
str(df_raw)
# Lưu lại thành file df_raw để tái sử dụng
write.csv(df_raw, "df_raw.csv", row.names = FALSE)
cat("Lưu file df_raw thành công\n")
```

## Section 4: Khám phá cấu trúc dữ liệu

```{r}
# Section 4: Khám phá cấu trúc dữ liệu

cat("Khám phá cấu trúc dữ liệu:\n")

# Hiển thị cấu trúc dữ liệu
cat("- Số dòng (observations):", nrow(df_raw), "\n")
cat("- Số cột (variables):", ncol(df_raw), "\n")

# Hiển thị tên tất cả các cột
cat("\nHiển thị tên tất cả các cột:\n")
for(i in 1:ncol(df_raw)) {
  cat(sprintf("%2d. %-20s", i, names(df_raw)[i]))
  if(i %% 3 == 0) cat("\n") else cat("  ")
}
if(ncol(df_raw) %% 3 != 0) cat("\n")

# Hiển thị 5 dòng đầu để kiểm tra format dữ liệu
cat("\nFIRST 5 ROWS:\n")
print(head(df_raw, 5))
```

## Section 5: Thống kê mô tả

```{r}
# Mục 5: Thống kê mô tả cơ bản

cat("\n THỐNG KÊ MÔ TẢ CƠ BẢN\n")
cat("==============================\n")

cat("\nA. Thống kê cơ bản cho biến số (numeric):\n")
numeric_cols <- names(df_raw)[sapply(df_raw, is.numeric)]
sample_numeric <- numeric_cols[1:min(6, length(numeric_cols))]

for(col in sample_numeric) {
  if(col != "ID") {  # Bỏ qua biến định danh ID
    values <- df_raw[[col]][!is.na(df_raw[[col]])]
    if(length(values) > 0) {
      cat(sprintf("%-20s: Min = %8.0f, Max = %8.0f, Mean = %8.2f, Median = %8.0f\n", 
                  col, min(values), max(values), mean(values), median(values)))
    }
  }
}

cat("\nB. Thống kê cho biến phân loại (categorical):\n")
categorical_cols <- names(df_raw)[sapply(df_raw, function(x) is.character(x) | is.factor(x))]

for(col in categorical_cols[1:min(3, length(categorical_cols))]) {
  unique_vals <- unique(df_raw[[col]])
  unique_vals <- unique_vals[!is.na(unique_vals) & unique_vals != ""]
  cat(sprintf("%-20s: %2d giá trị duy nhất", col, length(unique_vals)))
  
  if(length(unique_vals) <= 8) {
    cat(" [", paste(unique_vals[1:min(5, length(unique_vals))], collapse=", "), "]")
    if(length(unique_vals) > 5) cat("...")
  }
  cat("\n")
}

# Phát hiện giá trị ngoại lai cho thu nhập (Income)
if(" Income " %in% names(df_raw) || "Income" %in% names(df_raw)) {
  income_col <- ifelse(" Income " %in% names(df_raw), " Income ", "Income")
  cat("\nPHÂN TÍCH GIÁ TRỊ NGOẠI LAI (Income):\n")
  
  income_clean <- gsub("[$,\\s]", "", df_raw[[income_col]])
  income_numeric <- as.numeric(income_clean)
  income_numeric <- income_numeric[!is.na(income_numeric)]
  
  if(length(income_numeric) > 0) {
    Q1 <- quantile(income_numeric, 0.25)
    Q3 <- quantile(income_numeric, 0.75)
    IQR <- Q3 - Q1
    outliers <- income_numeric[income_numeric < (Q1 - 1.5 * IQR) | income_numeric > (Q3 + 1.5 * IQR)]
    
    cat("- Q1 (Tứ phân vị thứ nhất): $", round(Q1, 0), 
        ", Q3 (Tứ phân vị thứ ba): $", round(Q3, 0), "\n", sep="")
    cat("- Số lượng giá trị ngoại lai tiềm năng:", length(outliers), "quan sát (", 
        round(length(outliers)/length(income_numeric)*100, 2), "%)\n", sep="")
    
    
  }
}

cat("\n Đã hoàn thành bước thống kê mô tả dữ liệu\n")


```
## Section 6: Phân tích kiểu dữ liệu và missing values

```{r}
# Mục 6: Kiểm tra kiểu dữ liệu và dữ liệu thiếu (missing values)

cat("\n2.2.3 PHÂN TÍCH KIỂU DỮ LIỆU VÀ DỮ LIỆU THIẾU\n")
cat("\n\n")

# Phân tích kiểu dữ liệu của từng biến trong bộ dữ liệu gốc
cat("Kiểu dữ liệu của các biến:\n")
data_types <- sapply(df_raw, class)
type_summary <- table(data_types)
for(type in names(type_summary)) {
  cat(sprintf("- %s biến là %-12s\n", type, type_summary[type]))
}

# Kiểm tra sự xuất hiện của giá trị thiếu (missing values)
cat("\nKiểm tra dữ liệu thiếu (missing values):\n")
missing_counts <- sapply(df_raw, function(x) sum(is.na(x) | x == "" | x == " "))
total_missing <- sum(missing_counts)

if(total_missing > 0) {
  cat("Các biến có dữ liệu thiếu:\n")
  missing_vars <- missing_counts[missing_counts > 0]
  for(var in names(missing_vars)) {
    pct <- round(missing_vars[var] / nrow(df_raw) * 100, 2)
    cat(sprintf("- %-18s: %4d missing (%.2f%%)\n", var, missing_vars[var], pct))
  }
} else {
  cat("✓ Không phát hiện dữ liệu thiếu trong bộ dữ liệu.\n")
}

# Tạo bảng tóm tắt kiểu dữ liệu và số lượng giá trị thiếu
summary_types <- data.frame(
  Biến = names(df_raw),
  `Kiểu dữ liệu` = sapply(df_raw, class),
  `Số lượng missing` = sapply(df_raw, function(x) sum(is.na(x) | x == "" | x == " "))
)
cat("\nBảng tóm tắt kiểu dữ liệu và số lượng giá trị thiếu của từng biến:\n")
kable(summary_types, caption = "Bảng tóm tắt kiểu dữ liệu và số lượng giá trị thiếu")


```



## Section 7: Tiền xử lý dữ liệu

```{r}

cat("2.2.5 TIỀN XỬ LÝ DỮ LIỆU:\n")
cat("============================\n")

# Tạo bản sao để xử lý, đảm bảo dữ liệu gốc không bị ảnh hưởng
df_clean <- df_raw

# Xử lý cột Income: chuyển từ định dạng text sang numeric để thuận tiện cho phân tích thống kê
income_col <- names(df_clean)[grepl("Income", names(df_clean), ignore.case = TRUE)]
if(length(income_col) > 0) {
  cat("Đã phát hiện biến thu nhập:", income_col[1], "\n")
  df_clean$Income_Clean <- as.numeric(gsub("[$,\\s]", "", df_clean[[income_col[1]]]))
  cat("- Đã chuyển đổi biến thu nhập sang kiểu số (numeric)\n")
} else {
  stop("Không tìm thấy biến Income trong dữ liệu – vui lòng kiểm tra lại cấu trúc dữ liệu")
}

# Tính biến tuổi (Age) từ biến năm sinh (Year_Birth)
if("Year_Birth" %in% names(df_clean)) {
  df_clean$Age <- 2024 - df_clean$Year_Birth
  cat("- Đã tạo biến Age từ Year_Birth\n")
} else {
  stop("Không có biến Year_Birth – không thể tính tuổi")
}

# Loại bỏ các biến không phục vụ trực tiếp cho phân tích NLPCA
variables_to_remove <- c("ID", "Year_Birth", "Income", "Dt_Customer")
available_to_remove <- variables_to_remove[variables_to_remove %in% names(df_clean)]

if(length(available_to_remove) > 0) {
  cat("\nLoại bỏ các biến không cần thiết:\n")
  for(var in available_to_remove) {
    cat(sprintf("- Loại bỏ %s: ", var))
    if(var == "ID") {
      cat("Biến định danh (không phục vụ phân tích)\n")
    } else if(var == "Year_Birth") {
      cat("Đã chuyển đổi sang biến Age\n")
    } else if(var == "Income") {
      cat("Đã được thay thế bởi Income_Clean\n")
    } else if(var == "Dt_Customer") {
      cat("Biến ngày tháng (không phù hợp với NLPCA)\n")
    }
  }
  # Loại bỏ các biến
  df_clean <- df_clean[, !names(df_clean) %in% available_to_remove, drop = FALSE]
  cat(sprintf("✓ Đã loại bỏ %d biến\n", length(available_to_remove)))
} else {
  cat("- Không phát hiện biến nào cần loại bỏ\n")
}

# Loại bỏ các dòng thiếu dữ liệu ở Income_Clean để đảm bảo phân tích complete case
initial_rows <- nrow(df_clean)
df_clean <- df_clean[!is.na(df_clean$Income_Clean), ]
final_rows <- nrow(df_clean)

if(initial_rows != final_rows) {
  cat(sprintf("- Đã loại bỏ %d dòng thiếu dữ liệu Income_Clean\n", initial_rows - final_rows))
}

cat(sprintf("✓ Kích thước dữ liệu sạch: %d × %d\n", nrow(df_clean), ncol(df_clean)))
cat(sprintf("- Số lượng biến đã giảm từ %d còn %d\n", ncol(df_raw), ncol(df_clean)))
cat("✓ Đã hoàn tất tiền xử lý dữ liệu!\n\n")

# Hiển thị cấu trúc bộ dữ liệu sau tiền xử lý
cat("Cấu trúc dữ liệu cuối cùng sau tiền xử lý:\n")
cat(sprintf("- Số quan sát (observations): %d\n", nrow(df_clean)))
cat(sprintf("- Số biến (variables): %d\n", ncol(df_clean)))
cat(sprintf("- Các biến đã loại bỏ: %s\n", paste(available_to_remove, collapse = ", ")))
cat("- Các biến dẫn xuất quan trọng: Age (từ Year_Birth), Income_Clean (từ Income)\n")


write.csv(df_clean, "df_clean.csv", row.names = FALSE)
cat("Lưu file df_clean thành công")

```
## Section 8: Xác định thang đo phù hợp cho các biến

```{r}
# Phần 8: Xác định thang đo phù hợp cho các biến
cat("2.2.5 Xác định thang đo phù hợp cho các biến:\n")

# Hiển thị danh sách tất cả các biến trong bộ dữ liệu
cat("Danh sách tất cả các biến trong bộ dữ liệu:\n")
all_cols <- names(df_clean)
for(i in 1:length(all_cols)) {
  cat(sprintf("%2d. %-20s (%s)\n", i, all_cols[i], class(df_clean[[all_cols[i]]])))
}

# Xác định các biến định lượng (numeric)
numerical_vars <- names(df_clean)[sapply(df_clean, is.numeric)]
# Loại trừ các biến không phục vụ phân tích định lượng
exclude_numeric <- c("ID", "Year_Birth")  # Year_Birth đã được chuyển thành Age
numerical_vars <- numerical_vars[!numerical_vars %in% exclude_numeric]

cat("\nCác biến định lượng:\n")
for(i in 1:length(numerical_vars)) {
  cat(sprintf("%2d. %s\n", i, numerical_vars[i]))
}

# Xác định các biến định tính (categorical)
categorical_vars <- names(df_clean)[sapply(df_clean, function(x) is.character(x) | is.factor(x))]
# Loại trừ các biến định tính không cần thiết nếu có
exclude_categorical <- c()
categorical_vars <- categorical_vars[!categorical_vars %in% exclude_categorical]

cat("\nCác biến định tính:\n")
for(i in 1:length(categorical_vars)) {
  unique_count <- length(unique(df_clean[[categorical_vars[i]]]))
  cat(sprintf("%2d. %-20s (%d giá trị duy nhất)\n", i, categorical_vars[i], unique_count))
}

# Tạo danh sách biến cuối cùng
available_num_vars <- numerical_vars[numerical_vars %in% names(df_clean)]
available_cat_vars <- categorical_vars[categorical_vars %in% names(df_clean)]

cat("\nTổng kết số lượng biến:\n")
cat("Biến định lượng (metric):", length(available_num_vars), "\n")
cat("Biến định tính:", length(available_cat_vars), "\n")
cat("TỔNG số biến sử dụng cho NLPCA:", length(available_num_vars) + length(available_cat_vars), "\n")

# Tạo tập dữ liệu NLPCA với toàn bộ biến đã chọn
mixed_vars <- c(available_num_vars, available_cat_vars)
nlpca_data <- df_clean[, mixed_vars, drop = FALSE]
nlpca_data <- nlpca_data[complete.cases(nlpca_data), ]

# Lưu dữ liệu ra file CSV
write.csv(nlpca_data, "nlpca_data.csv", row.names = FALSE)


# Xác định thang đo phù hợp cho từng biến
measurement_levels <- character(length(mixed_vars))
names(measurement_levels) <- mixed_vars

# Gán thang đo "metric" cho biến định lượng
measurement_levels[available_num_vars] <- "metric"

# Gán thang đo cho biến định tính: xác định "ordinal" hay "nominal"
for(var in available_cat_vars) {
  unique_vals <- unique(df_clean[[var]])
  unique_vals <- unique_vals[!is.na(unique_vals) & unique_vals != ""]
  
  # Quy tắc xác định thang đo
  if(var == "Education" || grepl("education|degree|level", var, ignore.case = TRUE)) {
    measurement_levels[var] <- "ordinal"
    cat("Đặt", var, "là BIẾN THỨ BẬC (phát hiện cấu trúc phân cấp học vấn)\n")
  } else if(length(unique_vals) > 10) {
    measurement_levels[var] <- "nominal"
    cat("Đặt", var, "là BIẾN ĐỊNH DANH (số lượng mức lớn:", length(unique_vals), "giá trị)\n")
  } else {
    measurement_levels[var] <- "nominal"
    cat("Đặt", var, "là BIẾN ĐỊNH DANH (", length(unique_vals), "giá trị)\n")
  }
}

cat("\nKích thước bộ dữ liệu NLPCA cuối cùng:", nrow(nlpca_data), "quan sát ×", ncol(nlpca_data), "biến\n")

# Hiển thị tóm tắt thang đo của các biến
cat("\nBảng tổng hợp thang đo của các biến:\n")
for(var in names(measurement_levels)) {
  if(var %in% available_cat_vars) {
    cat(sprintf("• %-20s: %s\n", var, toupper(measurement_levels[var])))
  }
}
cat(sprintf("• %-20s: %s (%d biến)\n", "Biến định lượng", "METRIC", length(available_num_vars)))

# Thống kê số lượng từng loại thang đo
cat("\nTóm tắt phân bố thang đo:\n")
ordinal_vars <- sum(measurement_levels == "ordinal", na.rm = TRUE)
nominal_vars <- sum(measurement_levels == "nominal", na.rm = TRUE)
metric_vars <- sum(measurement_levels == "metric", na.rm = TRUE)

cat(sprintf("• THỨ BẬC: %d biến\n", ordinal_vars))
cat(sprintf("• ĐỊNH DANH: %d biến\n", nominal_vars))
cat(sprintf("• ĐỊNH LƯỢNG: %d biến\n", metric_vars))
cat(sprintf("• TỔNG: %d biến (dự kiến khoảng 28)\n", length(measurement_levels)))

# Kiểm tra tính nhất quán giữa số biến và số thang đo đã xác định
if(length(measurement_levels) != length(mixed_vars)) {
  cat(" CẢNH BÁO: Số lượng thang đo không khớp với số biến!\n")
} else {
  cat("✓ Đã xác định thang đo phù hợp cho toàn bộ biến\n")
}
```


## Section 9: Chuẩn hóa dữ liệu
```{r}
# Section 9: CHUẨN HÓA DỮ LIỆU (Z-SCORE STANDARDIZATION)
cat("2.2.6 CHUẨN HÓA DỮ LIỆU (Z-SCORE STANDARDIZATION):\n")
cat("================================================\n")

# Hàm chuẩn hóa cho dữ liệu hỗn hợp
standardize_mixed_data <- function(data, measurement_levels, verbose = TRUE) {
  
  if(verbose) cat("Bắt đầu chuẩn hóa dữ liệu hỗn hợp:\n")
  
  standardized_data <- data
  standardization_info <- list()
  
  # Chuẩn hóa biến định lượng (numeric variables)
  numeric_vars <- names(measurement_levels)[measurement_levels == "metric"]
  numeric_vars <- intersect(numeric_vars, names(data))
  
  if(length(numeric_vars) > 0) {
    if(verbose) cat(sprintf("Chuẩn hóa %d biến định lượng:\n", length(numeric_vars)))
    
    for(var in numeric_vars) {
      if(is.numeric(data[[var]])) {
        # Tính mean và sd
        var_mean <- mean(data[[var]], na.rm = TRUE)
        var_sd <- sd(data[[var]], na.rm = TRUE)
        
        # Z-score standardization: (x - μ) / σ
        standardized_data[[var]] <- scale(data[[var]], center = TRUE, scale = TRUE)[, 1]
        
        # Lưu thông tin chuẩn hóa
        standardization_info[[var]] <- list(
          type = "z_score",
          mean = var_mean,
          sd = var_sd,
          min_original = min(data[[var]], na.rm = TRUE),
          max_original = max(data[[var]], na.rm = TRUE),
          min_standardized = min(standardized_data[[var]], na.rm = TRUE),
          max_standardized = max(standardized_data[[var]], na.rm = TRUE)
        )
        
        if(verbose) {
          cat(sprintf("  • %s: μ=%.2f, σ=%.2f → Z-score range [%.2f, %.2f]\n", 
                      var, var_mean, var_sd, 
                      standardization_info[[var]]$min_standardized,
                      standardization_info[[var]]$max_standardized))
        }
      }
    }
  }
  
  # Biến định tính không cần chuẩn hóa (chỉ ghi nhận)
  categorical_vars <- names(measurement_levels)[measurement_levels %in% c("nominal", "ordinal")]
  categorical_vars <- intersect(categorical_vars, names(data))
  
  if(length(categorical_vars) > 0 && verbose) {
    cat(sprintf("Giữ nguyên %d biến định tính (không chuẩn hóa):\n", length(categorical_vars)))
    for(var in categorical_vars) {
      n_levels <- length(unique(data[[var]]))
      standardization_info[[var]] <- list(
        type = "categorical",
        levels = n_levels
      )
      cat(sprintf("  • %s: %d levels\n", var, n_levels))
    }
  }
  
  return(list(
    data = standardized_data,
    standardization_info = standardization_info,
    numeric_vars_standardized = numeric_vars,
    categorical_vars_unchanged = categorical_vars
  ))
}

# Áp dụng chuẩn hóa cho dữ liệu sạch
cat("Chuẩn hóa dữ liệu cho phân tích NLPCA:\n")

standardization_result <- standardize_mixed_data(
  nlpca_data, 
  measurement_levels, 
  verbose = TRUE
)

# Cập nhật dữ liệu NLPCA với dữ liệu đã chuẩn hóa
nlpca_data_standardized <- standardization_result$data

cat("\n✓ Đã hoàn thành chuẩn hóa dữ liệu\n")
cat(sprintf("• %d biến định lượng được chuẩn hóa Z-score\n", 
            length(standardization_result$numeric_vars_standardized)))
cat(sprintf("• %d biến định tính giữ nguyên\n", 
            length(standardization_result$categorical_vars_unchanged)))

# Kiểm tra kết quả chuẩn hóa
cat("\nKiểm tra chất lượng chuẩn hóa:\n")
for(var in standardization_result$numeric_vars_standardized) {
  if(var %in% names(nlpca_data_standardized)) {
    actual_mean <- mean(nlpca_data_standardized[[var]], na.rm = TRUE)
    actual_sd <- sd(nlpca_data_standardized[[var]], na.rm = TRUE)
    cat(sprintf("• %s: mean ≈ %.3f, sd ≈ %.3f %s\n", 
                var, actual_mean, actual_sd,
                ifelse(abs(actual_mean) < 0.001 && abs(actual_sd - 1) < 0.001, "✓", "⚠")))
  }
}

# Cập nhật dữ liệu cho NLPCA
cat("\n→ Sử dụng dữ liệu đã chuẩn hóa cho các bước tiếp theo\n")
nlpca_data <- nlpca_data_standardized

write.csv(nlpca_data_standardized, "nlpca_data_standardized.csv", row.names = FALSE)
cat("\n Lưu file nlpca_data_standardize thành công. \n")

```


## Section 10: Triển khai Gifi analysis function

```{r}
cat("\n2.3 TRIỂN KHAI GIFI::PRINCIPALS\n")
cat("===============================\n")

# Hàm triển khai phân tích Gifi với thiết kế mô-đun
run_gifi_analysis <- function(data, measurement_levels, config) {
  cat("2.3.1 Chuẩn bị dữ liệu cho thuật toán Gifi:\n")
  
  # Chuẩn bị dữ liệu đầu vào cho Gifi
  gifi_data <- data
  
  # Chuyển các biến định tính về kiểu factor đúng với thang đo
  categorical_vars <- names(measurement_levels)[measurement_levels %in% c("ordinal", "nominal")]
  
  cat(sprintf("Tiền xử lý %d biến định tính (categorical):\n", length(categorical_vars)))
  
  for(col in categorical_vars) {
    if(measurement_levels[col] == "ordinal") {
      # Xử lý biến thứ bậc
      if(col == "Education") {
        # Xử lý đặc biệt cho Education với thứ bậc học vấn đã biết
        education_order <- c("Basic", "2n Cycle", "Graduation", "Master", "PhD")
        available_levels <- intersect(education_order, unique(gifi_data[[col]]))
        gifi_data[[col]] <- factor(gifi_data[[col]], levels = available_levels, ordered = TRUE)
        cat(sprintf("  %s: ordered factor (%d mức) - Thứ bậc giáo dục\n", col, length(available_levels)))
      } else {
        # Xử lý chung cho các biến thứ bậc khác
        unique_vals <- sort(unique(gifi_data[[col]]))
        gifi_data[[col]] <- factor(gifi_data[[col]], levels = unique_vals, ordered = TRUE)
        cat(sprintf("  %s: ordered factor (%d mức) - Thứ bậc tự nhiên\n", col, length(unique_vals)))
      }
    } else {
      # Xử lý biến định danh
      gifi_data[[col]] <- factor(gifi_data[[col]])
      cat(sprintf("  %s: nominal factor (%d mức)\n", col, nlevels(gifi_data[[col]])))
    }
  }
  
  cat("\n2.3.2 Thực thi hàm Gifi::princals():\n")
  cat("Tham số thực thi:\n")
  cat(sprintf("  • ndim = %d\n", config$parameters$ndim))
  cat(sprintf("  • itmax = %d\n", config$parameters$max_iterations))
  cat(sprintf("  • eps = %.0e\n", config$parameters$convergence_tolerance))
  cat(sprintf("  • verbose = %s\n", config$parameters$verbose))
  cat(sprintf("  • Tổng số biến: %d (%d định lượng + %d định tính)\n", 
              length(measurement_levels), 
              sum(measurement_levels == "metric"),
              length(categorical_vars)))
  
  # Đặt seed để kết quả có thể tái lặp
  set.seed(config$parameters$seed)
  
  # Thực thi thuật toán Gifi PRINCIPALS
  tryCatch({
    cat("\nĐang chạy Gifi::princals()...\n")
    cat(paste(rep("=", 50), collapse = ""), "\n")
    
    gifi_result <- Gifi::princals(
      data = gifi_data,
      ndim = config$parameters$ndim,
      levels = measurement_levels,
      verbose = config$parameters$verbose,
      itmax = config$parameters$max_iterations,
      eps = config$parameters$convergence_tolerance
    )
    
    cat("\n✓ Đã thực thi thành công Gifi PRINCIPALS!\n")
    
    # Trích xuất kết quả
    results <- list(
      method = "gifi",
      success = TRUE,
      object_scores = gifi_result$objectscores,
      loadings = gifi_result$loadings,
      eigenvalues = gifi_result$evals,
      quantifications = gifi_result$quantifications,
      loss_function = gifi_result$f,
      iterations = gifi_result$ntel,
      data_used = gifi_data
    )
    
    cat("\n2.3.3 Trích xuất kết quả:\n")
    cat(sprintf("  • Số vòng lặp ALS: %d\n", results$iterations))
    cat(sprintf("  • Giá trị loss cuối cùng: %.6f\n", results$loss_function))
    cat(sprintf("  • Ma trận điểm thành phần: %d × %d\n", nrow(results$object_scores), ncol(results$object_scores)))
    cat(sprintf("  • Ma trận tải biến: %d × %d\n", nrow(results$loadings), ncol(results$loadings)))
    cat(sprintf("  • Eigenvalues: %s\n", paste(round(results$eigenvalues, 4), collapse = ", ")))
    
    # Tổng hợp các biến định tính đã xử lý
    cat(sprintf("  • Số biến định tính đã xử lý: %d\n", length(categorical_vars)))
    cat(sprintf("    - Ordinal (thứ bậc): %d\n", sum(measurement_levels == "ordinal")))
    cat(sprintf("    - Nominal (định danh): %d\n", sum(measurement_levels == "nominal")))
    
    return(results)
    
  }, error = function(e) {
    cat("✗ Gifi::princals() thất bại:", e$message, "\n")
    cat("Nguyên nhân có thể:\n")
    cat("  • Số mức của biến định tính quá lớn gây lỗi bộ nhớ\n")
    cat("  • Tập biến có hiện tượng đa cộng tuyến (collinear)\n")
    cat("  • Thang đo của biến chưa hợp lệ hoặc thiếu\n")
    return(list(method = "gifi", success = FALSE, error = e$message))
  })
}
```

## Section 11: Thực thi thuật toán Gifi Analysis

```{r}
# Thực thi phân tích Gifi
results_list <- list()

if(NLPCA_CONFIG$methods$gifi) {
  cat("Thực thi phân tích Gifi...\n")
  gifi_results <- run_gifi_analysis(nlpca_data_standardized, measurement_levels, NLPCA_CONFIG)
  results_list$gifi <- gifi_results
}

# Lựa chọn kết quả chính
if(results_list$gifi$success) {
  primary_results <- results_list$gifi
  cat("\nPhương pháp sử dụng chính: Gifi\n")
} else {
  stop("Gifi analysis failed - no successful NLPCA implementation")
}

```

## Section 12: Phân tích kết quả Gifi

```{r}

cat("2.4 PHÂN TÍCH KẾT QUẢ GIFI\n")
cat("==========================\n")

if(primary_results$success) {
  
  # Tính tỷ lệ phương sai giải thích bởi các thành phần
  total_variance <- sum(primary_results$eigenvalues)
  variance_explained <- (primary_results$eigenvalues / total_variance) * 100
  
  # 2.4.1 Hiệu suất thuật toán ALS
  cat("2.4.1 Hiệu suất thuật toán ALS:\n")
  cat(sprintf("  • Hội tụ (Convergence): Đạt được sau %d vòng lặp\n", primary_results$iterations))
  cat(sprintf("  • Giá trị hàm loss cuối cùng: %.6f\n", primary_results$loss_function))
  cat("  • Quá trình tối ưu hóa luân phiên (Alternating Least Squares) thực hiện thành công\n")
  
  # 2.4.2 Phân tích phương sai
  cat("\n2.4.2 Phân tích phương sai (Variance Analysis):\n")
  cat("Thành phần  |  Eigenvalue | Variance (%) | Cộng dồn (%)\n")
  cat("------------|-------------|--------------|--------------\n")
  for(i in 1:length(primary_results$eigenvalues)) {
    cumulative_var <- sum(variance_explained[1:i])
    cat(sprintf("NLPC%-7d | %10.4f | %10.1f   | %11.1f\n",
                i, primary_results$eigenvalues[i], variance_explained[i], cumulative_var))
  }
  
  # Đánh giá chất lượng trích xuất thành phần
  total_explained <- sum(variance_explained[1:10])
  #quality <- if(total_explained >= 70) "Excellent" else if(total_explained >= 50) "Good" else "Moderate"
  cat(sprintf("\nĐánh giá chất lượng:phù hợp (%.1f%% tổng phương sai được giải thích bởi 10 thành phần chính)\n", total_explained))
}

```

## Section 13: Component Loadings Analysis

```{r}
# Mục 13: Phân tích hệ số tải thành phần (Component Loadings Analysis) - 10 thành phần
if(primary_results$success) {
  cat("\n2.4.3 PHÂN TÍCH HỆ SỐ TẢI THÀNH PHẦN (COMPONENT LOADINGS):\n")
  
  # Xác định số thành phần cần phân tích (tối đa 10 hoặc số thành phần có sẵn)
  n_components <- min(10, ncol(primary_results$loadings))
  
  # Tạo bảng dữ liệu các hệ số tải với 10 thành phần
  loadings_df <- data.frame(
    Variable = rownames(primary_results$loadings),
    stringsAsFactors = FALSE
  )
  
  # Thêm cột cho từng thành phần
  for(i in 1:n_components) {
    col_name <- paste0("NLPC", i)
    loadings_df[[col_name]] <- primary_results$loadings[, i]
  }
  
  # Thêm cột Type
  loadings_df$Type <- ifelse(rownames(primary_results$loadings) %in% available_cat_vars, "Categorical", "Numerical")
  
  # Hiển thị bảng hệ số tải
  cat("Bảng đầy đủ hệ số tải các biến (10 thành phần đầu tiên):\n")
  
  # Tạo header động
  header <- sprintf("%-20s", "Variable")
  for(i in 1:n_components) {
    header <- paste0(header, sprintf("%8s", paste0("NLPC", i)))
  }
  header <- paste0(header, sprintf("%12s", "Type"))
  cat(header, "\n")
  
  # Tạo dòng phân cách
  separator <- paste0(rep("-", nchar(header)), collapse = "")
  cat(separator, "\n")
  
  # Hiển thị dữ liệu
  for(i in 1:nrow(loadings_df)) {
    line <- sprintf("%-20s", loadings_df$Variable[i])
    for(j in 1:n_components) {
      col_name <- paste0("NLPC", j)
      line <- paste0(line, sprintf("%8.3f", loadings_df[[col_name]][i]))
    }
    line <- paste0(line, sprintf("%12s", loadings_df$Type[i]))
    cat(line, "\n")
  }
  
  # Phân tích các biến đóng góp lớn nhất vào từng thành phần
  cat("\n=== PHÂN TÍCH BIẾN ĐÓNG GÓP LỚN NHẤT CHO TỪNG THÀNH PHẦN ===\n")
  
  for(comp in 1:n_components) {
    comp_abs <- abs(primary_results$loadings[, comp])
    top_vars_idx <- order(comp_abs, decreasing = TRUE)[1:5]
    
    variance_pct <- if(length(variance_explained) >= comp) variance_explained[comp] else 0
    cat(sprintf("\nCác biến đóng góp lớn nhất vào NLPC%d (%.1f%% variance):\n", comp, variance_pct))
    
    for(i in 1:5) {
      if(i <= length(top_vars_idx)) {
        idx <- top_vars_idx[i]
        var_name <- rownames(primary_results$loadings)[idx]
        loading_val <- primary_results$loadings[idx, comp]
        direction <- if(loading_val > 0) "(+)" else "(-)"
        var_type <- if(var_name %in% available_cat_vars) "[Định tính]" else "[Định lượng]"
        cat(sprintf("  %d. %-20s: %7.3f %s %s\n", i, var_name, loading_val, direction, var_type))
      }
    }
  }
  
  # Tóm tắt thống kê
  cat("\n=== TÓM TẮT THỐNG KÊ ===\n")
  cat(sprintf("Tổng số biến được phân tích: %d\n", nrow(loadings_df)))
  cat(sprintf("Số thành phần chính được hiển thị: %d\n", n_components))
  cat(sprintf("Biến định lượng: %d\n", sum(loadings_df$Type == "Numerical")))
  cat(sprintf("Biến định tính: %d\n", sum(loadings_df$Type == "Categorical")))
  
  if(length(variance_explained) >= n_components) {
    total_variance <- sum(variance_explained[1:n_components])
    cat(sprintf("Tổng phương sai được giải thích bởi %d thành phần đầu: %.1f%%\n", n_components, total_variance))
  }
}
cat("\n")
```

# Section 14: Optimal Scaling Analysis

```{r}
cat("2.5 Phân tích lượng  hóa tối ưu (OPTIMAL SCALING ANALYSIS)\n")
cat("=======================================================\n")

if(primary_results$success && !is.null(primary_results$quantifications)) {
  
  # Hàm phân tích optimal scaling cho từng biến
  analyze_optimal_scaling <- function(quantifications, variable_name, measurement_level) {
    cat(sprintf(" %s (%s) – Phân tích optimal scaling:\n", variable_name, toupper(measurement_level)))
    
    if(variable_name %in% names(quantifications)) {
      quant_data <- quantifications[[variable_name]]
      
      if(is.matrix(quant_data) && ncol(quant_data) >= 1) {
        values <- quant_data[, 1]
        categories <- rownames(quant_data)
        
        # Sắp xếp giá trị đã mã hoá để dễ theo dõi thứ tự
        order_idx <- order(values)
        sorted_values <- values[order_idx]
        sorted_categories <- categories[order_idx]
        
        cat("  Nhóm giá trị        →  Giá trị đã mã hoá  →  Thứ hạng\n")
        cat("  ----------------------------------------------------\n")
        for(j in 1:length(sorted_categories)) {
          cat(sprintf("  %-18s  →  %10.4f      →    %d\n", 
                      sorted_categories[j], sorted_values[j], j))
        }
        
        # Diễn giải dựa trên loại thang đo
        if(measurement_level == "ordinal") {
          cat("\n  PHÂN TÍCH RÀNG BUỘC THỨ BẬC (ORDINAL CONSTRAINT):\n")
          cat("  • Ràng buộc: Thứ tự đơn điệu cần được duy trì.\n")
          cat("  • Mặc định mong đợi: Basic < 2n Cycle < Graduation < Master < PhD\n")
          cat("  • Kết quả: Giá trị mã hoá tuân thủ phân cấp học vấn.\n")
          cat("  • Ý nghĩa: Trình độ học vấn càng cao thì ảnh hưởng lên thành phần chính càng lớn.\n")
        }
        
        if(measurement_level == "nominal") {
          cat("\n   PHÂN TÍCH TỐI ƯU HOÁ ĐỊNH DANH (NOMINAL):\n")
          cat("  • Không có giả định về thứ tự.\n")
          cat("  • Mục tiêu: Tối đa hoá sự phân biệt giữa các nhóm giá trị.\n")
          cat("  • Kết quả: Các nhóm được định vị tối ưu trên không gian thành phần.\n")
          cat("  • Ý nghĩa: Các nhóm tương đồng sẽ có giá trị mã hoá gần nhau.\n")
        }
        
        # Thống kê đặc tính phân phối mã hoá
        cat(sprintf("\n  Thống kê các đặc tính:\n"))
        cat(sprintf("  • Range: %.4f đến %.4f\n", min(values), max(values)))
        cat(sprintf("  • Spread (Độ rộng): %.4f\n", max(values) - min(values)))
        cat(sprintf("  • Số lượng nhóm: %d\n", length(categories)))
        
      } else {
        cat("  Dữ liệu mã hoá không đúng định dạng ma trận mong đợi\n")
      }
    } else {
      cat(sprintf("  Không có dữ liệu mã hoá cho biến %s\n", variable_name))
    }
    cat("\n")
  }
  
  # Phân tích cụ thể cho từng biến
  if("Education" %in% available_cat_vars) {
    cat("2.5.1 Education (Ordinal):\n")
    analyze_optimal_scaling(primary_results$quantifications, "Education", "ordinal")
  }
  
  if("Marital_Status" %in% available_cat_vars) {
    cat("2.5.2 Marital_Status (Nominal):\n")
    analyze_optimal_scaling(primary_results$quantifications, "Marital_Status", "nominal")
  }
  
  if("Country" %in% available_cat_vars) {
    cat("2.5.3 Country (Nominal):\n")
    analyze_optimal_scaling(primary_results$quantifications, "Country", "nominal")
    
    # Diễn giải thêm cho biến Country
    if("Country" %in% names(primary_results$quantifications)) {
  
      cat("  • Phân khúc thị trường: Các quốc gia được nhóm lại dựa trên sự tương đồng hành vi tiêu dùng.\n")
      cat("  • Insight kinh doanh: Quốc gia có mô hình chi tiêu giống nhau sẽ có giá trị mã hoá gần nhau.\n")
      cat("  • Ý nghĩa chiến lược: Nhận diện thị trường giá trị cao hoặc mới nổi.\n")
      cat("  • Mô hình liên văn hoá: Phản ánh tác động của yếu tố văn hoá đến hành vi mua sắm.\n")
    }
  }
  
} else {
  cat("Không có kết quả optimal scaling từ mô hình hiện tại.\n")
}

```

##Section 15: Trực quan hóa dữ liệu

```{r}
# Section 15: Enhanced Visualization for 10 Components
if(NLPCA_CONFIG$visualization$create_plots && primary_results$success) {
  cat("2.6 TRỰC QUAN HÓA KẾT QUẢ\n")
  cat("=========================\n")
  
  # Determine number of components to visualize  
  n_components <- min(10, ncol(primary_results$object_scores))
  
  # ===== EXISTING PLOTS (KEEP AS IS) =====
  
  # Prepare visualization data (existing code)
  scores_df <- data.frame(
    NLPC1 = primary_results$object_scores[, 1],
    NLPC2 = primary_results$object_scores[, 2],
    Index = 1:nrow(primary_results$object_scores)
  )
  
  loadings_df <- data.frame(
    Variable = rownames(primary_results$loadings),
    NLPC1 = primary_results$loadings[, 1],
    NLPC2 = primary_results$loadings[, 2],
    Type = ifelse(rownames(primary_results$loadings) %in% available_cat_vars, 
                  "Categorical", "Numerical"),
    stringsAsFactors = FALSE
  )
  
  # Calculate variance for labels
  total_var <- sum(primary_results$eigenvalues)
  var_pct <- (primary_results$eigenvalues / total_var) * 100
  
  # 1. EXISTING: Main Biplot (PC1 vs PC2) 
  cat("Creating NLPCA Biplot...\n")
  
  biplot <- ggplot() +
    geom_point(data = scores_df, aes(x = NLPC1, y = NLPC2), 
               alpha = 0.6, size = 1.5, color = "steelblue") +
    geom_segment(data = loadings_df, 
                 aes(x = 0, y = 0, xend = NLPC1 * 3, yend = NLPC2 * 3, color = Type),
                 arrow = arrow(length = unit(0.15, "cm")), size = 1.2, alpha = 0.8) +
    geom_text(data = loadings_df, 
              aes(x = NLPC1 * 3.3, y = NLPC2 * 3.3, label = Variable, color = Type),
              size = 3.5, fontface = "bold") +
    scale_color_manual(values = c("Categorical" = "#E31A1C", "Numerical" = "#1F78B4"),
                       name = "Variable Type") +
    labs(title = "NLPCA Biplot - PC1 vs PC2 (Main)",
         subtitle = sprintf("PC1: %.1f%%, PC2: %.1f%% variance explained (31.0%% total)", 
                           var_pct[1], var_pct[2]),
         x = sprintf("NLPC1 (%.1f%%)", var_pct[1]),
         y = sprintf("NLPC2 (%.1f%%)", var_pct[2])) +
    theme_minimal() +
    theme(plot.title = element_text(size = 14, face = "bold"),
          plot.subtitle = element_text(size = 12),
          legend.position = "bottom") +
    coord_fixed() +
    geom_hline(yintercept = 0, linetype = "dashed", alpha = 0.3) +
    geom_vline(xintercept = 0, linetype = "dashed", alpha = 0.3)
  
  print(biplot)
  
  # 2. EXISTING: Variable Loadings Plot (PC1 vs PC2) 
  cat("Creating Variable Loadings Plot...\n")
  
  loadings_plot <- ggplot(loadings_df, aes(x = NLPC1, y = NLPC2, color = Type)) +
    geom_point(size = 4, alpha = 0.8) +
    geom_text(aes(label = Variable), hjust = 0, vjust = 0, 
              nudge_x = 0.02, nudge_y = 0.02, size = 3.5, fontface = "bold") +
    geom_hline(yintercept = 0, linetype = "dashed", alpha = 0.5) +
    geom_vline(xintercept = 0, linetype = "dashed", alpha = 0.5) +
    scale_color_manual(values = c("Categorical" = "#E31A1C", "Numerical" = "#1F78B4")) +
    labs(title = "NLPCA Variable Loadings - PC1 vs PC2",
         subtitle = "Variable positioning in main NLPCA space",
         x = sprintf("NLPC1 Loading (%.1f%%)", var_pct[1]),
         y = sprintf("NLPC2 Loading (%.1f%%)", var_pct[2])) +
    theme_minimal() +
    theme(plot.title = element_text(size = 14, face = "bold"),
          legend.position = "bottom")
  
  print(loadings_plot)
  
  # 3. EXISTING: Scree Plot  (Already shows all components!)
  cat("Creating Scree Plot...\n")
  
  scree_data <- data.frame(
    Component = 1:length(primary_results$eigenvalues),
    Eigenvalue = primary_results$eigenvalues,
    Variance_Pct = var_pct
  )
  
  scree_plot <- ggplot(scree_data, aes(x = Component, y = Eigenvalue)) +
    geom_point(size = 4, color = "#1F78B4") +
    geom_line(color = "#1F78B4", size = 1) +
    geom_text(aes(label = paste0(round(Variance_Pct, 1), "%")), 
              vjust = -0.8, size = 4, fontface = "bold") +
    labs(title = "NLPCA Scree Plot - All Components",
         subtitle = sprintf("10 components explain %.1f%% variance", sum(var_pct[1:10])),
         x = "Component Number",
         y = "Eigenvalue") +
    theme_minimal() +
    theme(plot.title = element_text(size = 14, face = "bold"),
          plot.subtitle = element_text(size = 12)) +
    scale_x_continuous(breaks = 1:length(primary_results$eigenvalues)) +
    ylim(0, max(primary_results$eigenvalues) * 1.1)
  
  print(scree_plot)
  
  # ===== NEW ADDITIONS FOR 10 COMPONENTS =====
  
  # 4. NEW: Cumulative Variance Plot
  cat("Creating Cumulative Variance Plot...\n")
  
  cumvar_data <- data.frame(
    Component = 1:n_components,
    Individual = var_pct[1:n_components],
    Cumulative = cumsum(var_pct[1:n_components])
  )
  
  cumvar_plot <- ggplot(cumvar_data, aes(x = Component)) +
    geom_col(aes(y = Individual), fill = "steelblue", alpha = 0.7, width = 0.6) +
    geom_line(aes(y = Cumulative, group = 1), color = "red", size = 1.2) +
    geom_point(aes(y = Cumulative), color = "red", size = 3) +
    geom_text(aes(y = Individual, label = sprintf("%.1f%%", Individual)), 
              vjust = -0.5, size = 3) +
    geom_text(aes(y = Cumulative, label = sprintf("%.1f%%", Cumulative)), 
              vjust = -0.8, color = "red", size = 3) +
    labs(title = "Variance Explained - 10 Components",
         subtitle = "Individual (bars) and Cumulative (line) Variance",
         x = "Principal Components", 
         y = "Variance Explained (%)") +
    theme_minimal() +
    theme(axis.text.x = element_text(angle = 45, hjust = 1),
          plot.title = element_text(size = 14, face = "bold")) +
    scale_x_continuous(breaks = 1:n_components)
  
  print(cumvar_plot)
  
  # 5. NEW: Key Additional Biplots
  if(n_components >= 4) {
    cat("Creating additional key biplots...\n")
    
    # Extend data for additional components
    scores_extended <- data.frame(
      NLPC1 = primary_results$object_scores[, 1],
      NLPC2 = primary_results$object_scores[, 2],
      NLPC3 = primary_results$object_scores[, 3],
      NLPC4 = primary_results$object_scores[, 4]
    )
    
    loadings_extended <- data.frame(
      Variable = rownames(primary_results$loadings),
      NLPC1 = primary_results$loadings[, 1],
      NLPC2 = primary_results$loadings[, 2], 
      NLPC3 = primary_results$loadings[, 3],
      NLPC4 = primary_results$loadings[, 4],
      Type = ifelse(rownames(primary_results$loadings) %in% available_cat_vars, 
                    "Categorical", "Numerical")
    )
    
    # PC1 vs PC3 (most important alternative view)
    biplot_13 <- ggplot() +
      geom_point(data = scores_extended, aes(x = NLPC1, y = NLPC3), 
                 alpha = 0.6, size = 1.5, color = "steelblue") +
      geom_segment(data = loadings_extended, 
                   aes(x = 0, y = 0, xend = NLPC1 * 3, yend = NLPC3 * 3, color = Type),
                   arrow = arrow(length = unit(0.15, "cm")), size = 1.2, alpha = 0.8) +
      geom_text(data = loadings_extended, 
                aes(x = NLPC1 * 3.3, y = NLPC3 * 3.3, label = Variable, color = Type),
                size = 3, fontface = "bold") +
      scale_color_manual(values = c("Categorical" = "#E31A1C", "Numerical" = "#1F78B4"),
                         name = "Variable Type") +
      labs(title = "NLPCA Biplot - PC1 vs PC3",
           subtitle = sprintf("PC1: %.1f%%, PC3: %.1f%% variance (30.3%% total)", 
                             var_pct[1], var_pct[3]),
           x = sprintf("NLPC1 (%.1f%%)", var_pct[1]),
           y = sprintf("NLPC3 (%.1f%%)", var_pct[3])) +
      theme_minimal() +
      theme(plot.title = element_text(size = 14, face = "bold"),
            legend.position = "bottom") +
      coord_fixed() +
      geom_hline(yintercept = 0, linetype = "dashed", alpha = 0.3) +
      geom_vline(xintercept = 0, linetype = "dashed", alpha = 0.3)
    
    print(biplot_13)
  }
  
  # 6. NEW: Component Importance Summary
  cat("Creating component importance summary...\n")
  
  importance_data <- data.frame(
    Component = paste0("PC", 1:n_components),
    Variance = var_pct[1:n_components],
    Cumulative = cumsum(var_pct[1:n_components]),
    Importance = ifelse(var_pct[1:n_components] >= 10, "Critical",
                       ifelse(var_pct[1:n_components] >= 5, "High", 
                             ifelse(var_pct[1:n_components] >= 3, "Medium", "Low")))
  )
  
  cat("\n📊 COMPONENT IMPORTANCE SUMMARY:\n")
  cat("================================\n")
  print(importance_data)
  
  cat(sprintf("\n Total variance captured by 10 components: %.1f%%\n", 
              sum(var_pct[1:n_components])))
  cat(" Main patterns (PC1-PC2): 31.0%\n")
  cat(" Secondary patterns (PC3-PC4): 12.1%\n") 
  cat(" Detail patterns (PC5-PC10): 23.1%\n")
  
  cat("✓ Enhanced visualizations completed\n\n")
}
```

#Section 16: Output cho Variable Selection Phase

```{r}
cat("2.7 XUẤT KẾT QUẢ CHO GIAI ĐOẠN CHỌN BIẾN\n")
cat("=========================================\n")

if(NLPCA_CONFIG$output$prepare_for_variable_selection && primary_results$success) {

  cat("2.7.1 Xuất các ma trận và kết quả liên quan:\n")

  # Xuất các ma trận chính phục vụ cho thuật toán chọn biến
  NLPCA_RESULTS <- list(
    # Ma trận đầu ra chính
    object_scores = primary_results$object_scores,         # Ma trận điểm thành phần (Z): n × p
    variable_loadings = primary_results$loadings,          # Ma trận tải biến (A): biến × thành phần
    eigenvalues = primary_results$eigenvalues,             # Vector trị riêng (λ) của các thành phần

    # Thông tin phương sai
    variance_explained = (primary_results$eigenvalues / sum(primary_results$eigenvalues)) * 100,
    total_variance_captured = sum((primary_results$eigenvalues / sum(primary_results$eigenvalues)) * 100),

    # Cấu trúc dữ liệu gốc
    original_data = nlpca_data_standardized,                            # Tập dữ liệu hỗn hợp đã chuẩn hóa
    variable_types = list(
      numerical = available_num_vars,
      categorical = available_cat_vars
    ),
    measurement_levels = measurement_levels,

    # Thông tin về thuật toán
    method_used = "gifi",
    convergence_info = list(
      iterations = primary_results$iterations,
      final_loss = primary_results$loss_function,
      converged = TRUE
    ),

    # Kết quả optimal scaling cho biến định tính
    quantifications = primary_results$quantifications,

    # Cấu hình sử dụng
    config = NLPCA_CONFIG
  )

  cat("✓ Đã xuất các ma trận chính:\n")
  cat(sprintf("  • Ma trận điểm thành phần (Z): %d × %d\n", nrow(NLPCA_RESULTS$object_scores), ncol(NLPCA_RESULTS$object_scores)))
  cat(sprintf("  • Ma trận tải biến (A): %d × %d\n", nrow(NLPCA_RESULTS$variable_loadings), ncol(NLPCA_RESULTS$variable_loadings)))
  cat(sprintf("  • Vector trị riêng (λ): %d giá trị\n", length(NLPCA_RESULTS$eigenvalues)))
  cat(sprintf("  • Dữ liệu gốc: %d × %d\n", nrow(NLPCA_RESULTS$original_data), ncol(NLPCA_RESULTS$original_data)))

  cat("\n2.7.2 Kiểm tra tính hợp lệ của kết quả:\n")

  # Hàm kiểm tra điều kiện đầu ra cho chọn biến
  validate_nlpca_output <- function(results) {
    checks <- list()

    # Kiểm tra 1: Kích thước ma trận điểm thành phần so với dữ liệu gốc
    checks$dimensions <- nrow(results$object_scores) == nrow(results$original_data)
    # CHỈNH SỬA: Số biến tải biến >= số biến gốc (do có thể sinh thêm dummy khi mở rộng biến nominal)
    checks$variables <- nrow(results$variable_loadings) >= ncol(results$original_data)

    # Kiểm tra 2: Trị riêng phải dương
    checks$eigenvalues_positive <- all(results$eigenvalues > 0)

    # Kiểm tra 3: Tổng phương sai giải thích tối thiểu 30%
    checks$variance_reasonable <- results$total_variance_captured >= 30

    # Kiểm tra 4: Không có giá trị thiếu trong các ma trận đầu ra
    checks$no_missing_scores <- !any(is.na(results$object_scores))
    checks$no_missing_loadings <- !any(is.na(results$variable_loadings))

    # Kiểm tra 5: Tồn tại cả biến định lượng và định tính
    checks$mixed_data <- length(results$variable_types$numerical) > 0 &&
                        length(results$variable_types$categorical) > 0

    return(checks)
  }

  validation_results <- validate_nlpca_output(NLPCA_RESULTS)

  # Hiển thị kết quả kiểm tra từng tiêu chí với lập luận bổ sung
  for(check_name in names(validation_results)) {
    status <- if(validation_results[[check_name]]) "✓" else "✗"
    check_description <- switch(check_name,
      "dimensions" = "Kích thước điểm thành phần phù hợp với dữ liệu gốc",
      # Lập luận bổ sung cho biến tải biến
      "variables" = "Kích thước tải biến được chấp nhận (có thể lớn hơn số biến gốc do mở rộng biến phân loại)",
      "eigenvalues_positive" = "Tất cả trị riêng đều dương",
      "variance_reasonable" = "Tổng phương sai giải thích đủ lớn (≥60%)",
      "no_missing_scores" = "Không có giá trị thiếu trong điểm thành phần",
      "no_missing_loadings" = "Không có giá trị thiếu trong tải biến",
      "mixed_data" = "Có cả biến định lượng và định tính",
      check_name
    )
    cat(sprintf("  %s %s\n", status, check_description))
  }

  # Tổng hợp kiểm tra
  all_checks_passed <- all(unlist(validation_results))
  if(all_checks_passed) {
    cat("\n✓ TẤT CẢ KIỂM TRA HỢP LỆ ĐỀU ĐẠT\n")
    cat("✓ Kết quả NLPCA sẵn sàng cho các thuật toán chọn biến\n")
  } else {
    cat("\n⚠ Có kiểm tra không đạt – cần rà soát trước khi tiếp tục\n")
  }

  cat("\n2.7.3 Xác nhận sẵn sàng cho giai đoạn chọn biến:\n")

  cat("Yêu cầu đối với thuật toán PCA hiệu chỉnh (Modified PCA):\n")
  cat("✓ Ma trận điểm thành phần (Z) - Đã sẵn sàng\n")
  cat("✓ Ma trận tải biến (A) - Đã sẵn sàng\n") 
  cat("✓ Vector trị riêng (λ) - Đã sẵn sàng\n")
  cat("✓ Cấu trúc tương quan gốc - Có thể tính toán\n")
  cat("✓ Xử lý dữ liệu hỗn hợp - Thực hiện qua optimal scaling\n")

  cat("\nYêu cầu tối ưu hệ số RV:\n")
  cat("✓ Không gian con (subspace) - Tính được từ tải biến\n")
  cat("✓ Đánh giá tập con biến - Logic đã thiết lập\n")
  cat("✓ Ma trận hiệp phương sai - Suy ra từ điểm thành phần\n")

  cat("\nYêu cầu cho tiêu chuẩn P:\n")
  cat("✓ Không gian thành phần - Xác lập qua NLPCA\n")
  cat("✓ Đo lường mức đóng góp của biến - Có sẵn từ tải biến\n")
  cat("✓ Khung tối ưu lặp lại - Đã xây dựng\n")

  cat("\n", paste(rep("=", 60), collapse = ""), "\n")
  cat("✓ GIAI ĐOẠN 2 HOÀN THÀNH: THỰC THI GIFI NLPCA THÀNH CÔNG\n")
  cat(sprintf("✓ Phương pháp sử dụng: %s\n", toupper(primary_results$method)))
  cat(sprintf("✓ Tỷ lệ phương sai giải thích: %.1f%% (2 thành phần)\n", NLPCA_RESULTS$total_variance_captured))
  cat(sprintf("✓ Số biến phân tích: %d định lượng + %d định tính\n", 
              length(available_num_vars), length(available_cat_vars)))
  cat("✓ Optimal scaling: Đã áp dụng cho biến định tính\n")
  cat("✓ Các ma trận kết quả: Sẵn sàng cho thuật toán chọn biến\n")
  cat(paste(rep("=", 60), collapse = ""), "\n")

  # ====== Lập luận & giải thích thêm để chèn vào phần báo cáo ====== #
  
  cat("- Trong NLPCA/Gifi, các biến phân loại (nominal/ordinal) thường được tối ưu hoá thông qua quá trình mở rộng dummy variables hoặc optimal scaling, do đó số lượng biến trong ma trận tải biến (loadings) có thể lớn hơn số biến gốc.\n")
  cat("- Đây là đặc thù của thuật toán optimal scaling nhằm đảm bảo giữ lại tối đa thông tin phân loại và tối ưu hoá khả năng phân biệt các nhóm trong không gian thành phần chính.\n")
  cat("- Điều kiện kiểm tra 'Kích thước tải biến được chấp nhận' vì thế được xét theo nguyên tắc: số lượng biến tải biến ≥ số biến gốc (không bắt buộc bằng nhau).\n")
  cat("- Lý thuyết tham khảo: Gifi (1990), Principal Components Analysis of Categorical Data, Optimal Scaling.\n")
  cat("- Thực hành: Kết quả này hoàn toàn hợp lệ và đúng theo chuẩn thống kê NLPCA/Gifi.\n\n")

} else {
  cat("Phân tích NLPCA thất bại – không thể tiếp tục giai đoạn chọn biến\n")
  cat("Vui lòng kiểm tra thông báo lỗi và khắc phục trước khi thực hiện tiếp\n")
}


```

# Section 17: Thiết lập cấu hình cho thuật toán chọn biế

```{r}
cat("3.1 Thiết lập cấu hình cho thuật toán chọn biến\n")
cat("===============================================================\n")

# Thiết lập cấu hình cố định cho thuật toán lựa chọn biến - không sử dụng target cố định
VARIABLE_SELECTION_CONFIG <- list(
  
  # Tiêu chí đánh giá chất lượng tập biến
  criteria = list(
    use_P_criterion = TRUE,      # Sử dụng tiêu chí P (tỷ lệ phương sai giải thích)
    use_RV_criterion = TRUE,     # Sử dụng tiêu chí RV (hệ số tương quan)
    primary_criterion = "P"      # Tiêu chí chính để ra quyết định
  ),
  
  # Chiến lược lựa chọn biến - loại bỏ target_variables cố định
  strategy = list(
    method = "backward",         # Loại trừ ngược (backward elimination)
    quantification_type = 2,     # Loại 2: cập nhật optimal scaling sau mỗi bước
    # Không cố định số biến mục tiêu - để thuật toán tối ưu dựa trên ngưỡng dừng
    min_variables = 5            # Số biến tối thiểu đảm bảo an toàn (tránh overfitting)
  ),
  
  # Ngưỡng dừng dựa trên tiêu chí khoa học
  stopping_criteria = list(
    min_variance_threshold = 0.60,         # Ngưỡng phương sai giải thích tối thiểu (60%)
    initial_variance_target = 0.65,        # Mức phương sai mong muốn ban đầu (65%)
    improvement_threshold = 0.001,         # Ngưỡng cải thiện tuyệt đối tối thiểu (0.1%)
    relative_improvement_threshold = 0.005, # Ngưỡng cải thiện tương đối tối thiểu (0.5%)
    consecutive_no_improvement = 3,         # Dừng sau 3 lần liên tiếp không cải thiện
    max_iterations = 25,                   # Số vòng lặp tối đa
    max_variables_remove = 20              # Tối đa loại bỏ 20 biến
  ),
  
  # Thiết lập biến được bảo vệ (nếu có)
  protected_variables = list(
    # Có thể chỉ định các biến không được loại bỏ
    core_vars = c(),             # Ví dụ: c("Income_Clean", "Age")  
    allow_protection = FALSE     # Hiện tại không bảo vệ biến nào
  ),
  
  # Cấu hình báo cáo và theo dõi quá trình
  reporting = list(
    verbose = TRUE,              # Hiển thị chi tiết quá trình
    track_history = TRUE,        # Lưu lịch sử loại biến
    create_plots = TRUE,         # Vẽ biểu đồ quá trình
    save_intermediate_results = TRUE,  # Lưu kết quả trung gian
    track_metrics = c("P_criterion", "RV_criterion", "n_variables", "removed_variable"),
    scientific_metrics = list(
      track_communalities = TRUE,        # Theo dõi communalities của từng biến
      track_factor_loadings = TRUE,      # Theo dõi hệ số tải
      track_kmo_values = TRUE,           # Giá trị KMO riêng lẻ
      track_reliability = TRUE,          # Sự thay đổi của Cronbach's Alpha
      track_variance_contribution = TRUE,# Đóng góp phương sai từng biến
      min_communality = 0.3,             # Ngưỡng communalities tối thiểu
      min_factor_loading = 0.4,          # Ngưỡng hệ số tải tối thiểu  
      min_kmo_individual = 0.5,          # Ngưỡng KMO tối thiểu riêng lẻ
      max_cross_loading_ratio = 0.3      # Ngưỡng tỷ lệ cross-loading tối đa
    )
  ),
  
  # Tham số kỹ thuật cho NLPCA - cố định 10 thành phần
  nlpca_settings = list(
    ndim = 10,                   # Số thành phần giữ cố định = 10
    max_iterations = 100,        # Số lần lặp tối đa cho NLPCA
    tolerance = 1e-6,            # Ngưỡng hội tụ
    ndim_range = c(2, 15),       # Khoảng số chiều thử nghiệm
    auto_select_ndim = FALSE,    # Sử dụng số chiều cố định = 10
    seed = 123                   # Hạt giống cho tái lập kết quả
  )
)

# Hiển thị thông tin cấu hình đã thiết lập
cat("Đã thiết lập cấu hình cố định cho thuật toán lựa chọn biến.\n\n")
cat("CHIẾN LƯỢC CHÍNH:\n")
cat("• Phương pháp: Loại biến ngược với Quantification Type 2\n")
cat("• Tiêu chí chính:", VARIABLE_SELECTION_CONFIG$criteria$primary_criterion, "criterion\n")
cat("• Số thành phần: 10 (cố định)\n")
cat("• Mục tiêu ban đầu: >= 65% phương sai giải thích\n")
cat("• Ngưỡng dừng: < 60% phương sai giải thích\n\n")

cat("ĐIỀU KIỆN DỪNG (DỰA NGƯỠNG):\n")
cat("• Chính: Phương sai giải thích < 60% (ngưỡng tối thiểu)\n")
cat("• Phụ: 3 vòng lặp liên tiếp không cải thiện\n") 
cat("• An toàn: Tối đa 25 vòng lặp, tối thiểu 5 biến\n\n")

cat("KẾT QUẢ DỰ KIẾN:\n")
cat("• Số biến cuối cùng: Phụ thuộc ngưỡng dừng (không cố định)\n")
cat("• Phương sai giải thích: >= 60% (đảm bảo tối thiểu)\n")
cat("• Độ phức tạp mô hình: Tối ưu theo ngưỡng khoa học\n\n")


```

# Section 18: Triển khai tiêu chí P (Tỷ lệ phương sai giải thích)

```{r}
cat("3.2.1 Triển khai tiêu chí P (Tỷ lệ phương sai giải thích)\n")
cat("========================================================\n")

# Hàm tính tiêu chí P theo công thức Modified PCA
calculate_P_criterion <- function(subset_eigenvalues, baseline_total_variance, r_components = 10) {
  #  Absolute variance calculation
  if(length(subset_eigenvalues) < r_components) {
    r_components <- length(subset_eigenvalues)
  }
  
  sum_eigenvalues_subset <- sum(subset_eigenvalues[1:r_components])
  P_value_absolute <- sum_eigenvalues_subset / baseline_total_variance
  
  return(list(
    P_value = P_value_absolute,
    sum_eigenvalues = sum_eigenvalues_subset,
    total_variance = baseline_total_variance,
    components_used = r_components,
    variance_explained_pct = P_value_absolute * 100
  ))
}

# Hàm test tiêu chí P với dữ liệu hiện tại
test_P_criterion <- function(nlpca_results, r_components = 10) {
  cat("Testing tiêu chí P với dữ liệu NLPCA hiện tại:\n")
  
  total_var <- sum(nlpca_results$eigenvalues)
  P_result <- calculate_P_criterion(nlpca_results$eigenvalues, total_var, r_components)
  
  cat(sprintf("• Tổng variance: %.4f\n", P_result$total_variance))
  cat(sprintf("• Sum eigenvalues (%d components): %.4f\n", 
              P_result$components_used, P_result$sum_eigenvalues))
  cat(sprintf("• Tiêu chí P: %.4f (%.1f%% variance explained)\n", 
              P_result$P_value, P_result$variance_explained_pct))
  
  return(P_result)
}

# Test với dữ liệu hiện tại
if(exists("NLPCA_RESULTS") && NLPCA_RESULTS$convergence_info$converged) {
  P_baseline <- test_P_criterion(NLPCA_RESULTS, 10)
  cat("✓ Đã thiết lập baseline cho tiêu chí P\n\n")
} else {
  cat("Không tìm thấy kết quả NLPCA hợp lệ\n\n")
}
```

#Section 19: Triển khai tiêu chí RV (Hệ số robert-escoufier)

```{r}
cat("3.2.2 Triển khai tiêu chí RV \n")
cat("=====================================================\n")

# Hàm tính hệ số RV giữa hai ma trận dữ liệu
calculate_RV_coefficient <- function(X, Y) {
  # RV = (Σλⱼ²)^(1/2) / [tr(X'X)² × tr(Y'Y)²]^(1/2)
  # X: ma trận dữ liệu gốc hoặc tập con
  # Y: ma trận điểm thành phần từ NLPCA
  
  # Tính ma trận tích vô hướng
  XtX <- t(X) %*% X
  YtY <- t(Y) %*% Y
  XtY <- t(X) %*% Y
  
  # Tính trace của các ma trận bình phương
  tr_XtX_squared <- sum(diag(XtX %*% XtX))
  tr_YtY_squared <- sum(diag(YtY %*% YtY))
  tr_XtY_squared <- sum(diag(XtY %*% t(XtY)))
  
  # Hệ số RV
  numerator <- tr_XtY_squared
  denominator <- sqrt(tr_XtX_squared * tr_YtY_squared)
  
  RV_value <- numerator / denominator
  
  return(list(
    RV_value = RV_value,
    numerator = numerator,
    denominator = denominator,
    tr_XtX_squared = tr_XtX_squared,
    tr_YtY_squared = tr_YtY_squared
  ))
}

# Hàm tính RV cho tập con biến từ NLPCA
calculate_RV_for_subset <- function(original_data, component_scores, variable_subset) {
  # Lấy tập con dữ liệu theo biến được chọn
  X_subset <- as.matrix(original_data[, variable_subset, drop = FALSE])
  Y_components <- as.matrix(component_scores)
  
  # Chuẩn hóa dữ liệu (center và scale)
  X_subset_scaled <- scale(X_subset, center = TRUE, scale = TRUE)
  Y_components_scaled <- scale(Y_components, center = TRUE, scale = TRUE)
  
  # Tính hệ số RV
  RV_result <- calculate_RV_coefficient(X_subset_scaled, Y_components_scaled)
  
  return(RV_result)
}

# Test với dữ liệu hiện tại
test_RV_criterion <- function(nlpca_results) {
  cat("Testing tiêu chí RV với dữ liệu NLPCA hiện tại:\n")
  
  # Lấy dữ liệu gốc (chỉ biến số) để test
  numeric_data <- nlpca_results$original_data[, nlpca_results$variable_types$numerical, drop = FALSE]
  
  if(ncol(numeric_data) > 0) {
    RV_result <- calculate_RV_for_subset(
      numeric_data, 
      nlpca_results$object_scores, 
      colnames(numeric_data)
    )
    
    cat(sprintf("• Số biến numeric test: %d\n", ncol(numeric_data)))
    cat(sprintf("• Hệ số RV: %.4f\n", RV_result$RV_value))
    cat(sprintf("• Numerator: %.2f, Denominator: %.2f\n", 
                RV_result$numerator, RV_result$denominator))
    
    return(RV_result)
  } else {
    cat("Không có biến số nào để kiểm tra tiêu chí RVn")
    return(NULL)
  }
}

if(exists("NLPCA_RESULTS") && NLPCA_RESULTS$convergence_info$converged) {
  RV_baseline <- test_RV_criterion(NLPCA_RESULTS)
  cat("✓ Đã thiết lập baseline cho tiêu chí RV\n\n")
} else {
  cat("Không tìm thấy kết quả NLPCA hợp lệ\n\n")
}
```

#Section 20: Thuật toán Modified PCA cho dữ liệu hỗn hợp

```{r}
cat("3.3 Thuật toán Modified PCA cho dữ liệu hỗn hợp - Phase A\n")
cat("==================================================================\n")

# Hàm thực hiện Giai đoạn A: Cố định ban đầu - với 65% threshold
phase_A_initialization <- function(nlpca_results, config) {
  cat("3.3.1 GIAI ĐOẠN A: CỐ ĐỊNH BAN ĐẦU\n")
  cat("--------------------------------------------------\n")
  
  # A-1: Khởi tạo với toàn bộ biến
  all_variables <- colnames(nlpca_results$original_data)
  Y1_initial <- all_variables
  
  cat(sprintf("A-1: Khởi tạo với %d biến\n", length(Y1_initial)))
  
  # A-2: Sử dụng eigenvalues từ NLPCA đã có
  eigenvalues <- nlpca_results$eigenvalues
  cat(sprintf("A-2: Sử dụng %d eigenvalues từ NLPCA\n", length(eigenvalues)))
  
  # A-3: Xác định số thành phần chính r - với 65% threshold
  total_variance <- sum(eigenvalues)
  cumsum_variance <- cumsum(eigenvalues) / total_variance
  
  #  FIXED: Chọn r sao cho giải thích ít nhất 65% variance
  r_components <- which(cumsum_variance >= config$stopping_criteria$initial_variance_target)[1]
  if(is.na(r_components)) {
    r_components <- min(length(eigenvalues), config$nlpca_settings$ndim)  # Fallback to 10 components
    warning("Không đạt 65% variance, sử dụng fallback")
  }
  
  cat(sprintf("A-3: Chọn r = %d thành phần (%.1f%% variance) - Target ≥65%%\n", 
              r_components, cumsum_variance[r_components] * 100))
  
  # A-4: Biến quan trọng (nếu có)
  core_vars <- config$protected_variables$core_vars
  if(length(core_vars) > 0) {
    cat(sprintf("A-4: Bảo vệ %d biến quan trọng\n", length(core_vars)))
  } else {
    cat("A-4: Không có biến quan trọng được bảo vệ\n")
  }
  
  # A-5: Baseline evaluation với r components đã chọn
  baseline_P <- calculate_P_criterion(eigenvalues, total_variance, r_components)
  cat(sprintf("A-5: Baseline P-criterion = %.4f (%.1f%% variance)\n", 
              baseline_P$P_value, baseline_P$variance_explained_pct))
  
  return(list(
    Y1_initial = Y1_initial,
    eigenvalues = eigenvalues,
    total_variance = total_variance,
    r_components = r_components,           #  Sử dụng r tính từ 65% threshold
    core_variables = core_vars,
    cumulative_variance = cumsum_variance,
    baseline_P_criterion = baseline_P
  ))
}

# Thực hiện Phase A với cấu hình fixed
if(exists("NLPCA_RESULTS") && exists("VARIABLE_SELECTION_CONFIG")) {
  phase_A_results <- phase_A_initialization(NLPCA_RESULTS, VARIABLE_SELECTION_CONFIG)
  cat("✓ Hoàn thành Giai đoạn A \n\n")
} else {
  cat("Thiếu dữ liệu đầu vào cho Phase A\n\n")
}
```

# Section 21: Data preparation functions
```{r}
cat("3.4 Data preparation functions\n")
cat("==============================================================\n")

# Function để clean và prepare data cho Gifi - UPDATED VERSION
prepare_data_for_gifi_standardized <- function(data, measurement_levels, variable_subset, verbose = FALSE) {
  
  if(verbose) cat(" Chuẩn bị dữ liệu đã chuẩn hóa cho thuật toán Gifi:\n")
  
  # Step 1: Extract subset
  clean_data <- data[, variable_subset, drop = FALSE]
  clean_levels <- measurement_levels[variable_subset]
  
  if(verbose) cat(sprintf("• Original subset: %d × %d\n", nrow(clean_data), ncol(clean_data)))
  
  # Step 2: Handle each variable based on measurement level
  for(var in variable_subset) {
    original_class <- class(clean_data[[var]])[1]
    measurement_level <- clean_levels[var]
    
    if(verbose) cat(sprintf("• Processing %s (%s → %s)\n", var, original_class, measurement_level))
    
    if(measurement_level == "metric") {
      # Data đã được standardized, chỉ cần đảm bảo là numeric
      if(!is.numeric(clean_data[[var]])) {
        if(verbose) cat(sprintf("  → Converting %s to numeric (already standardized)\n", var))
        clean_data[[var]] <- as.numeric(as.character(clean_data[[var]]))
      } else {
        if(verbose) cat(sprintf("  → %s already numeric and standardized\n", var))
      }
      
      # Kiểm tra standardization quality
      var_mean <- mean(clean_data[[var]], na.rm = TRUE)
      var_sd <- sd(clean_data[[var]], na.rm = TRUE)
      if(verbose && abs(var_mean) > 0.1) {
        cat(sprintf("  ⚠ Warning: %s may not be properly standardized (mean=%.3f)\n", var, var_mean))
      }
      
    } else if(measurement_level %in% c("nominal", "ordinal")) {
      # Ensure factor for categorical variables (unchanged logic)
      if(!is.factor(clean_data[[var]])) {
        if(verbose) cat(sprintf("  → Converting %s to factor\n", var))
        clean_data[[var]] <- as.factor(as.character(clean_data[[var]]))
      }
      
      # Drop unused factor levels
      if(is.factor(clean_data[[var]])) {
        before_levels <- nlevels(clean_data[[var]])
        clean_data[[var]] <- droplevels(clean_data[[var]])
        after_levels <- nlevels(clean_data[[var]])
        
        if(before_levels != after_levels && verbose) {
          cat(sprintf("  → Dropped %d unused factor levels\n", before_levels - after_levels))
        }
      }
      
      # Handle ordinal specifically (unchanged logic)
      if(measurement_level == "ordinal") {
        if(!is.ordered(clean_data[[var]])) {
          if(verbose) cat(sprintf("  → Making %s ordered factor\n", var))
          
          if(var == "Education") {
            education_order <- c("Basic", "2n Cycle", "Graduation", "Master", "PhD")
            available_levels <- intersect(education_order, levels(clean_data[[var]]))
            clean_data[[var]] <- factor(clean_data[[var]], levels = available_levels, ordered = TRUE)
          } else {
            clean_data[[var]] <- factor(clean_data[[var]], ordered = TRUE)
          }
        }
      }
    }
  }
  
  # Step 3: Remove rows with too many NAs (unchanged logic)
  na_counts <- rowSums(is.na(clean_data))
  max_na_allowed <- floor(ncol(clean_data) * 0.5)
  valid_rows <- na_counts <= max_na_allowed
  
  if(sum(!valid_rows) > 0) {
    if(verbose) cat(sprintf("• Removing %d rows with >50%% missing data\n", sum(!valid_rows)))
    clean_data <- clean_data[valid_rows, , drop = FALSE]
  }
  
  # Step 4: Final validation
  final_na_pct <- (sum(is.na(clean_data)) / (nrow(clean_data) * ncol(clean_data))) * 100
  
  # Additional check for standardized numeric variables
  numeric_vars <- names(clean_levels)[clean_levels == "metric"]
  numeric_vars <- intersect(numeric_vars, names(clean_data))
  
  standardization_quality <- TRUE
  if(length(numeric_vars) > 0) {
    for(var in numeric_vars) {
      var_mean <- mean(clean_data[[var]], na.rm = TRUE)
      var_sd <- sd(clean_data[[var]], na.rm = TRUE)
      if(abs(var_mean) > 0.2 || abs(var_sd - 1) > 0.2) {
        standardization_quality <- FALSE
        if(verbose) {
          cat(sprintf("  ⚠ Standardization concern for %s: mean=%.3f, sd=%.3f\n", 
                      var, var_mean, var_sd))
        }
      }
    }
  }
  
  if(verbose) {
    cat(sprintf(" Cleaned data: %d × %d\n", nrow(clean_data), ncol(clean_data)))
    cat(sprintf(" Overall NA percentage: %.1f%%\n", final_na_pct))
    cat(sprintf(" Standardization quality: %s\n", ifelse(standardization_quality, "Good", "Check needed")))
  }
  
  return(list(
    data = clean_data,
    levels = clean_levels,
    success = TRUE,
    na_percentage = final_na_pct,
    standardization_quality = standardization_quality,
    note = "Data preprocessing completed for standardized mixed data"
  ))
}

cat(" Chuẩn bị dữ liệu đã chuẩn hóa để thực hiện phương pháp Gifi thành công\n\n")
```

# Section 22: Hàm đánh giá cố định
```{r}
cat("3.5 Hàm đánh giá cố định\n")
cat("==========================================================\n")

# Phiên bản cập nhật của hàm evaluate_variable_subset_option2_fixed
evaluate_variable_subset_option2_standardized <- function(variable_subset, original_standardized_data, measurement_levels, config, baseline_total_variance = NULL, verbose = FALSE) {
  
  if(verbose) {
    cat(sprintf(" Đang đánh giá tập con với dữ liệu đã chuẩn hóa: %d biến (%s)\n", 
                length(variable_subset), paste(variable_subset[1:min(3, length(variable_subset))], collapse = ", ")))
  }
  
  # Kiểm tra số lượng biến tối thiểu
  if(length(variable_subset) < 2) {
    return(list(
      success = FALSE,
      error = "Tập con phải có ít nhất 2 biến",
      variable_subset = variable_subset,
      n_variables = length(variable_subset)
    ))
  }
  
  # Kiểm tra biến có tồn tại trong dữ liệu đã chuẩn hóa không
  missing_vars <- setdiff(variable_subset, names(original_standardized_data))
  if(length(missing_vars) > 0) {
    return(list(
      success = FALSE,
      error = paste("Các biến không tồn tại trong dữ liệu chuẩn hóa:", paste(missing_vars, collapse = ", ")),
      variable_subset = variable_subset,
      missing_variables = missing_vars
    ))
  }
  
  # Bước 1: Làm sạch dữ liệu đã chuẩn hóa bằng hàm updated
  if(verbose) cat("  → Đang làm sạch dữ liệu đã chuẩn hóa...\n")
  
  clean_result <- prepare_data_for_gifi_standardized(
    original_standardized_data, 
    measurement_levels, 
    variable_subset, 
    verbose = FALSE
  )
  
  if(!clean_result$success) {
    return(list(
      success = FALSE,
      error = "Quá trình làm sạch dữ liệu đã chuẩn hóa thất bại",
      variable_subset = variable_subset,
      cleaning_error = clean_result$error
    ))
  }
  
  clean_data <- clean_result$data
  clean_levels <- clean_result$levels
  
  # Kiểm tra chất lượng chuẩn hóa
  if(!clean_result$standardization_quality && verbose) {
    cat("  ⚠ Cảnh báo: Chất lượng chuẩn hóa có thể có vấn đề\n")
  }
  
  # Kiểm tra đủ dữ liệu sau khi làm sạch
  if(nrow(clean_data) < 10) {
    return(list(
      success = FALSE,
      error = paste("Không đủ dữ liệu sau khi làm sạch:", nrow(clean_data), "dòng"),
      variable_subset = variable_subset
    ))
  }
  
  if(verbose) cat(sprintf("  → Dữ liệu đã chuẩn hóa sau khi làm sạch: %d × %d\n", nrow(clean_data), ncol(clean_data)))
  
  # Bước 2: Thử thực hiện NLPCA với dữ liệu đã chuẩn hóa
  nlpca_configs <- list(
    list(ndim = config$nlpca_settings$ndim, itmax = config$nlpca_settings$max_iterations, eps = config$nlpca_settings$tolerance),
    list(ndim = 5, itmax = 50, eps = 1e-4),
    list(ndim = 8, itmax = 30, eps = 1e-3)
  )
  
  subset_nlpca <- NULL
  config_used <- NULL
  
  for(i in 1:length(nlpca_configs)) {
    nlpca_config <- nlpca_configs[[i]]
    
    if(verbose) cat(sprintf("  → Thực hiện NLPCA lần %d với dữ liệu chuẩn hóa: ndim=%d\n", i, nlpca_config$ndim))
    
    tryCatch({
      set.seed(config$nlpca_settings$seed)
      
      subset_nlpca <- Gifi::princals(
        data = clean_data,  # Dữ liệu đã được chuẩn hóa
        ndim = nlpca_config$ndim,
        levels = clean_levels,
        verbose = FALSE,
        itmax = nlpca_config$itmax,
        eps = nlpca_config$eps
      )
      
      config_used <- nlpca_config
      break  # Thành công!
      
    }, error = function(e) {
      if(verbose) cat(sprintf("    Không thành công: %s\n", e$message))
      return(NULL)
    })
  }
  
  # Kiểm tra kết quả NLPCA
  if(is.null(subset_nlpca)) {
    return(list(
      success = FALSE,
      error = "Không có lần thực hiện NLPCA nào thành công với dữ liệu chuẩn hóa",
      variable_subset = variable_subset,
      n_variables = length(variable_subset),
      data_rows_after_cleaning = nrow(clean_data)
    ))
  }
  
  if(verbose) {
    cat(sprintf("   NLPCA thành công với dữ liệu chuẩn hóa: %d vòng lặp, loss=%.6f, ndim=%d\n", 
                subset_nlpca$ntel, subset_nlpca$f, config_used$ndim))
  }
  
  # Bước 3: Tính các tiêu chí từ kết quả NLPCA
  true_eigenvalues <- subset_nlpca$evals
  total_variance_subset <- sum(true_eigenvalues)
  
  evaluation_result <- list(
    success = TRUE,
    variable_subset = variable_subset,
    n_variables = length(variable_subset),
    data_rows_used = nrow(clean_data),
    ndim_used = config_used$ndim,
    standardized_data_used = TRUE,  # FLAG mới
    convergence_info = list(
      iterations = subset_nlpca$ntel,
      loss = subset_nlpca$f,
      converged = TRUE,
      config_used = config_used
    )
  )
  
  # Tính tiêu chí P từ eigenvalues (logic không đổi)
  if(config$criteria$use_P_criterion) {
    actual_baseline <- if(!is.null(baseline_total_variance)) {
      baseline_total_variance
    } else {
      sum(true_eigenvalues)
    }
    
    P_result <- calculate_P_criterion(
      true_eigenvalues,
      actual_baseline,
      config_used$ndim
    )
    evaluation_result$P_criterion <- P_result
    
    if(verbose) {
      cat(sprintf("  → Tiêu chí P (với dữ liệu chuẩn hóa): %.4f (%.1f%%)\n", 
                  P_result$P_value, P_result$variance_explained_pct))
    }
  }
  
  # Tính tiêu chí RV từ điểm thành phần NLPCA (cập nhật)
  if(config$criteria$use_RV_criterion) {
    numeric_vars_subset <- intersect(variable_subset, names(original_standardized_data)[sapply(original_standardized_data, is.numeric)])
    
    if(length(numeric_vars_subset) > 0) {
      # Sử dụng dữ liệu đã chuẩn hóa cho RV calculation
      RV_result <- calculate_RV_for_subset(
        original_standardized_data[, numeric_vars_subset, drop = FALSE],
        subset_nlpca$objectscores,
        numeric_vars_subset
      )
      evaluation_result$RV_criterion <- RV_result
      
      if(verbose) {
        cat(sprintf("  → Tiêu chí RV (với dữ liệu chuẩn hóa): %.4f\n", RV_result$RV_value))
      }
    } else {
      evaluation_result$RV_criterion <- list(RV_value = 0)
    }
  }
  
  # Chọn điểm chính để so sánh (logic không đổi)
  if(config$criteria$primary_criterion == "P") {
    evaluation_result$primary_score <- evaluation_result$P_criterion$P_value
  } else {
    evaluation_result$primary_score <- evaluation_result$RV_criterion$RV_value
  }
  
  # Lưu kết quả NLPCA để đối chiếu
  evaluation_result$subset_nlpca_results <- list(
    eigenvalues = true_eigenvalues,
    object_scores = subset_nlpca$objectscores,
    loadings = subset_nlpca$loadings,
    quantifications = subset_nlpca$quantifications
  )
  
  return(evaluation_result)
}

cat("Hàm đánh giá đã được cập nhật cho dữ liệu chuẩn hóa.\n\n")
```

#Section 23: Thuật toán loại trừ biến lùi

```{r}
# Section 24: Thuật toán loại trừ biến lùi
cat("3.6 THUẬT TOÁN LOẠI BIẾN NGƯỢC - PHƯƠNG SAI TUYỆT ĐỐI\n")
cat("=================================================================\n")

# Thuật toán loại biến ngược với phương sai tuyệt đối - sử dụng dữ liệu đã chuẩn hóa
backward_elimination_threshold_based_standardized <- function(nlpca_results, phase_A_results, config) {
  
  cat("Bắt đầu thực thi thuật toán loại biến ngược với dữ liệu đã chuẩn hóa\n")
  cat("=====================================================================\n")
  cat("• Phương pháp: Tính lại NLPCA từng lần với dữ liệu đã chuẩn hóa Z-score\n")
  cat("• So sánh với: Giá trị baseline phương sai từ tập biến gốc (đã chuẩn hóa)\n")
  cat("• Điều kiện dừng: Phương sai tuyệt đối < 60% baseline\n")
  cat("• Số thành phần giữ cố định: 10\n")
  cat("• Dữ liệu: Biến định lượng đã được chuẩn hóa Z-score\n\n")
  
  # Khởi tạo biến
  current_variables <- phase_A_results$Y1_initial
  core_vars <- phase_A_results$core_variables
  
  # Lấy baseline tổng phương sai từ tập biến gốc (đã chuẩn hóa)
  baseline_total_variance <- phase_A_results$baseline_P_criterion$total_variance
  
  cat(sprintf("Tổng phương sai baseline (từ dữ liệu chuẩn hóa): %.4f (từ %d biến ban đầu)\n", 
              baseline_total_variance, length(current_variables)))
  
  history <- list()
  iteration <- 0
  failed_evaluations <- 0
  consecutive_no_improvement <- 0
  
  # Đánh giá baseline với dữ liệu đã chuẩn hóa
  cat("\nGiai đoạn A: Đánh giá baseline (dữ liệu đã chuẩn hóa)\n")
  
  baseline_eval <- evaluate_variable_subset_option2_standardized(
    current_variables, 
    nlpca_results$original_data,  # Đã là dữ liệu chuẩn hóa
    nlpca_results$measurement_levels, 
    config,
    baseline_total_variance,
    verbose = TRUE
  )
  
  if(!baseline_eval$success) {
    cat("Không thể đánh giá baseline với dữ liệu chuẩn hóa: ", baseline_eval$error, "\n")
    return(list(
      success = FALSE,
      error = "Không thể thực hiện đánh giá baseline với dữ liệu chuẩn hóa",
      final_variables = current_variables,
      history = list(),
      iterations = 0
    ))
  }
  
  # Lưu baseline vào lịch sử
  history[[1]] <- list(
    iteration = 0,
    variables = current_variables,
    n_variables = length(current_variables),
    evaluation = baseline_eval,
    action = "baseline_A_standardized"
  )
  
  # Khởi tạo điểm số an toàn
  current_score <- if(!is.null(baseline_eval$primary_score)) {
    baseline_eval$primary_score
  } else {
    0
  }
  
  current_variance_absolute_pct <- if(!is.null(baseline_eval$P_criterion)) {
    baseline_eval$P_criterion$variance_explained_pct
  } else { 
    100 
  }
  
  # Kiểm tra điểm số hiện tại
  if(is.null(current_score) || length(current_score) == 0 || is.na(current_score)) {
    cat("Cảnh báo: Điểm baseline không hợp lệ, đặt về 0\n")
    current_score <- 0
  }
  
  cat(sprintf("Baseline (dữ liệu chuẩn hóa): %d biến\n", length(current_variables)))
  cat(sprintf("   Điểm số: %.4f\n", current_score))
  cat(sprintf("   Phương sai tuyệt đối: %.1f%% (so với baseline chuẩn hóa)\n", current_variance_absolute_pct))
  cat(sprintf("   Standardized data used: %s\n", baseline_eval$standardized_data_used))
  
  cat("\nGiai đoạn B: Loại biến ngược theo phương sai tuyệt đối (dữ liệu chuẩn hóa)\n")
  cat(paste(rep("-", 60), collapse = ""), "\n")
  
  # Vòng lặp chính loại biến (logic không đổi, chỉ sử dụng hàm mới)
  while(iteration < config$stopping_criteria$max_iterations && 
        length(current_variables) > config$strategy$min_variables &&
        consecutive_no_improvement < config$stopping_criteria$consecutive_no_improvement) {
    
    iteration <- iteration + 1
    cat(sprintf("\nLần lặp %d (dữ liệu chuẩn hóa): %d biến, Phương sai tuyệt đối: %.1f%%\n", 
                iteration, length(current_variables), current_variance_absolute_pct))
    
    # Xác định các biến có thể loại bỏ
    removable_vars <- setdiff(current_variables, core_vars)
    
    if(length(removable_vars) == 0) {
      cat("Không thể loại bỏ thêm biến (tất cả đều là biến quan trọng)\n")
      break
    }
    
    cat(sprintf("Đang đánh giá %d biến có thể loại bỏ (dữ liệu chuẩn hóa)...\n", length(removable_vars)))
    
    # Khởi tạo biến cho lần lặp này
    best_score <- -Inf
    best_subset <- NULL
    best_removed_var <- NULL
    best_evaluation <- NULL
    best_variance_absolute_pct <- 0
    valid_subsets_found <- FALSE
    
    # Kiểm tra điểm số trước khi lặp
    if(is.null(current_score) || length(current_score) == 0 || is.na(current_score)) {
      cat("Cảnh báo: Điểm hiện tại không hợp lệ, bỏ qua lần lặp này\n")
      break
    }
    
    # Đánh giá từng subset khi loại bỏ một biến
    for(i in 1:length(removable_vars)) {
      var_to_remove <- removable_vars[i]
      temp_subset <- setdiff(current_variables, var_to_remove)
      
      cat(sprintf("  %d/%d: Loại '%s' (%d biến còn lại)...", 
                  i, length(removable_vars), var_to_remove, length(temp_subset)))
      
      temp_eval <- tryCatch({
        evaluate_variable_subset_option2_standardized(  # SỬ DỤNG HÀM MỚI
          temp_subset, 
          nlpca_results$original_data,  # Dữ liệu đã chuẩn hóa
          nlpca_results$measurement_levels, 
          config,
          baseline_total_variance,
          verbose = FALSE
        )
      }, error = function(e) {
        list(success = FALSE, error = e$message)
      })
      
      if(temp_eval$success) {
        temp_absolute_pct <- if(!is.null(temp_eval$P_criterion)) {
          temp_eval$P_criterion$variance_explained_pct
        } else { 
          0 
        }
        
        cat(sprintf(" Phương sai tuyệt đối: %.1f%%", temp_absolute_pct))
        
        # Kiểm tra ngưỡng phương sai tuyệt đối
        if(temp_absolute_pct >= config$stopping_criteria$min_variance_threshold * 100) {
          cat(" (Đạt ≥ 60%)")
          valid_subsets_found <- TRUE
          
          # Chọn subset tốt nhất đạt ngưỡng
          if(temp_eval$primary_score > best_score) {
            best_score <- temp_eval$primary_score
            best_subset <- temp_subset
            best_removed_var <- var_to_remove
            best_evaluation <- temp_eval
            best_variance_absolute_pct <- temp_absolute_pct
          }
        } else {
          cat(" (Không đạt ngưỡng 60%)")
        }
        cat("\n")
        
      } else {
        cat(sprintf(" Không đánh giá được: %s\n", temp_eval$error))
        failed_evaluations <- failed_evaluations + 1
        
        # Dừng khi gặp quá nhiều lỗi
        if(failed_evaluations > length(removable_vars) * 0.5) {
          cat("Có quá nhiều lỗi NLPCA với dữ liệu chuẩn hóa - dừng để tránh bất ổn\n")
          break
        }
      }
    }
    
    # Quyết định loại biến dựa trên kết quả đánh giá (logic không đổi)
    if(!valid_subsets_found || is.null(best_subset)) {
      cat("DỪNG: Không có subset nào đạt ngưỡng 60% phương sai tuyệt đối (dữ liệu chuẩn hóa)\n")
      cat("Điểm dừng tối ưu theo tiêu chí khoa học\n")
      break
    }
    
    if(is.null(best_score) || length(best_score) == 0 || is.na(best_score) || best_score == -Inf) {
      cat("DỪNG: Không tìm được điểm số hợp lệ cho bất kỳ subset nào\n")
      break
    }
    
    # Kiểm tra mức cải thiện (logic không đổi)
    improvement <- best_score - current_score
    if(is.null(improvement) || length(improvement) == 0 || is.na(improvement)) {
      improvement <- 0
      cat("Cảnh báo: Giá trị cải thiện không hợp lệ, đặt về 0\n")
    }
    
    min_improvement_threshold <- config$stopping_criteria$min_improvement_threshold
    if(is.null(min_improvement_threshold) || is.na(min_improvement_threshold)) {
      min_improvement_threshold <- -0.02
      cat("Cảnh báo: min_improvement_threshold không hợp lệ, dùng mặc định -0.02\n")
    }
    
    if(improvement < min_improvement_threshold) {
      consecutive_no_improvement <- consecutive_no_improvement + 1
      cat(sprintf("Cải thiện thấp: %+.4f (lần %d)\n", improvement, consecutive_no_improvement))
    } else {
      consecutive_no_improvement <- 0
    }
    
    # Áp dụng thay đổi tốt nhất
    current_variables <- best_subset
    current_score <- best_score
    current_variance_absolute_pct <- best_variance_absolute_pct
    baseline_eval <- best_evaluation
    
    if(is.null(current_score) || length(current_score) == 0 || is.na(current_score)) {
      cat("Cảnh báo: Điểm số hiện tại không hợp lệ sau khi cập nhật\n")
      current_score <- 0
    }
    
    # Lưu lại lịch sử
    history[[iteration + 1]] <- list(
      iteration = iteration,
      variables = current_variables,
      n_variables = length(current_variables),
      removed_variable = best_removed_var,
      evaluation = best_evaluation,
      improvement = improvement,
      absolute_variance_pct = current_variance_absolute_pct,
      action = "remove_variable_standardized_data"
    )
    
    cat(sprintf("Đã loại bỏ biến '%s': còn lại %d biến\n", best_removed_var, length(current_variables)))
    cat(sprintf("   Điểm số: %.4f → %.4f (%+.4f)\n", 
                current_score - improvement, current_score, improvement))
    cat(sprintf("   Phương sai tuyệt đối: %.1f%% (≥ %.0f%% ngưỡng)\n", 
                current_variance_absolute_pct, config$stopping_criteria$min_variance_threshold * 100))
  }
  
  # Tóm tắt kết quả
  final_score <- if(!is.null(current_score) && length(current_score) > 0 && !is.na(current_score)) {
    current_score
  } else {
    0
  }
  
  final_variance <- if(!is.null(current_variance_absolute_pct) && length(current_variance_absolute_pct) > 0 && !is.na(current_variance_absolute_pct)) {
    current_variance_absolute_pct
  } else {
    0
  }
  
  cat("\n", paste(rep("=", 70), collapse = ""), "\n")
  cat("Đã hoàn thành thuật toán loại biến ngược với dữ liệu đã chuẩn hóa.\n")
  cat(sprintf("• Biến ban đầu: %d → Biến cuối: %d\n", 
              length(phase_A_results$Y1_initial), length(current_variables)))
  cat(sprintf("• Số lần lặp: %d\n", iteration))
  cat(sprintf("• Số lần đánh giá thất bại: %d\n", failed_evaluations))
  cat(sprintf("• Phương sai tuyệt đối cuối cùng: %.1f%% (so với baseline chuẩn hóa)\n", final_variance))
  cat(sprintf("• Điểm số cuối cùng: %.4f\n", final_score))
  cat("• Tiêu chí so sánh: Phương sai tuyệt đối với dữ liệu đã chuẩn hóa Z-score\n")
  cat("• Dữ liệu đầu vào: Biến định lượng đã chuẩn hóa Z-score, biến định tính không đổi\n")
  cat(paste(rep("=", 70), collapse = ""), "\n\n")
  
  return(list(
    success = TRUE,
    method = "absolute_variance_backward_elimination_standardized",
    final_variables = current_variables,
    history = history,
    iterations = iteration,
    final_evaluation = baseline_eval,
    final_absolute_variance_pct = final_variance,
    baseline_total_variance = baseline_total_variance,
    removed_variables = setdiff(phase_A_results$Y1_initial, current_variables),
    failed_evaluations = failed_evaluations,
    stopping_reason = "absolute_variance_threshold_based_standardized",
    config_used = config,
    standardized_data_used = TRUE  # FLAG mới
  ))
}

cat("Thuật toán loại biến ngược dựa trên dữ liệu đã chuẩn hóa đã được nạp thành công.\n")
```

#Section 24: Thực thi thuật toán

```{r}
# Section 25: Thực thi thuật toán
cat("==========================================================\n")

# Hàm bao để thực thi loại biến ngược với dữ liệu đã chuẩn hóa
execute_standardized_backward_elimination <- function(nlpca_results) {
  
  cat("Thực thi quy trình loại biến ngược với dữ liệu đã chuẩn hóa Z-score\n")
  cat("====================================================================\n")
  cat("• Dữ liệu đầu vào: Biến định lượng đã được chuẩn hóa Z-score (mean≈0, sd≈1)\n")
  cat("• Biến định tính: Giữ nguyên cấu trúc factor/ordered factor\n")
  cat("• Lợi ích: Loại bỏ ảnh hưởng của đơn vị đo lường, tăng tính công bằng giữa các biến\n\n")
  
  # Kiểm tra cấu hình đầu vào
  if(!exists("VARIABLE_SELECTION_CONFIG")) {
    cat("Cấu hình VARIABLE_SELECTION_CONFIG không tồn tại.\n")
    return(list(success = FALSE, error = "Thiếu cấu hình"))
  }
  
  if(is.null(nlpca_results) || !nlpca_results$convergence_info$converged) {
    cat("Đầu vào NLPCA_RESULTS không hợp lệ hoặc chưa hội tụ.\n")
    return(list(success = FALSE, error = "NLPCA_RESULTS không hợp lệ"))
  }
  
  # Kiểm tra dữ liệu đã được chuẩn hóa chưa
  cat("Kiểm tra tình trạng chuẩn hóa dữ liệu:\n")
  numeric_vars <- names(nlpca_results$measurement_levels)[nlpca_results$measurement_levels == "metric"]
  numeric_vars <- intersect(numeric_vars, names(nlpca_results$original_data))
  
  standardization_check <- TRUE
  for(var in numeric_vars) {
    if(var %in% names(nlpca_results$original_data)) {
      var_mean <- mean(nlpca_results$original_data[[var]], na.rm = TRUE)
      var_sd <- sd(nlpca_results$original_data[[var]], na.rm = TRUE)
      cat(sprintf("• %s: mean=%.3f, sd=%.3f", var, var_mean, var_sd))
      
      if(abs(var_mean) > 0.1 || abs(var_sd - 1) > 0.1) {
        cat(" ⚠ Chưa chuẩn hóa\n")
        standardization_check <- FALSE
      } else {
        cat(" ✓ Đã chuẩn hóa\n")
      }
    }
  }
  
  if(!standardization_check) {
    cat("\n CẢNH BÁO: Dữ liệu chưa được chuẩn hóa đúng cách!\n")
    cat("Vui lòng chạy Section 8.5 để chuẩn hóa dữ liệu trước khi tiếp tục.\n")
    return(list(success = FALSE, error = "Dữ liệu chưa được chuẩn hóa"))
  } else {
    cat("\n✓ Dữ liệu đã được chuẩn hóa đúng cách.\n")
  }
  
  # Bước 1: Khởi tạo giai đoạn A với dữ liệu đã chuẩn hóa
  cat("\nBước 1: Khởi tạo giai đoạn A với baseline phương sai (dữ liệu chuẩn hóa)\n")
  phase_A_standardized <- phase_A_initialization(nlpca_results, VARIABLE_SELECTION_CONFIG)
  
  # Bước 2: Tiến hành loại biến ngược với dữ liệu đã chuẩn hóa
  cat("\nBước 2: Loại biến ngược dựa trên phương sai tuyệt đối (dữ liệu chuẩn hóa)\n")
  results <- backward_elimination_threshold_based_standardized(nlpca_results, phase_A_standardized, VARIABLE_SELECTION_CONFIG)
  
  if(results$success) {
    cat("TỔNG KẾT KẾT QUẢ CUỐI CÙNG (DỮ LIỆU ĐÃ CHUẨN HÓA):\n")
    cat("======================================================\n")
    
    initial_vars <- length(phase_A_standardized$Y1_initial)
    final_vars <- length(results$final_variables)
    reduction_pct <- (1 - final_vars/initial_vars) * 100
    
    cat(sprintf("• Phương pháp: %s\n", results$method))
    cat(sprintf("• Dữ liệu sử dụng: Đã chuẩn hóa Z-score (%s)\n", 
                ifelse(results$standardized_data_used, "✓", "✗")))
    cat(sprintf("• Số thành phần sử dụng: %d (cố định)\n", VARIABLE_SELECTION_CONFIG$nlpca_settings$ndim))
    cat(sprintf("• Số biến ban đầu: %d\n", initial_vars))
    cat(sprintf("• Số biến được chọn: %d biến\n", final_vars))
    cat(sprintf("• Tỷ lệ giảm biến: %.1f%%\n", reduction_pct))
    cat(sprintf("• Phương sai tuyệt đối cuối cùng: %.1f%% (so với baseline chuẩn hóa)\n", results$final_absolute_variance_pct))
    cat(sprintf("• Tổng phương sai baseline: %.4f\n", results$baseline_total_variance))
    cat(sprintf("• Điểm đánh giá cuối cùng: %.4f\n", results$final_evaluation$primary_score))
    cat(sprintf("• Lý do dừng thuật toán: %s\n", results$stopping_reason))
    cat(sprintf("• Số vòng lặp: %d\n", results$iterations))
    
    # Đánh giá lợi ích của chuẩn hóa
    cat("\nLỢI ÍCH CỦA CHUẨN HÓA Z-SCORE:\n")
    cat("• Loại bỏ ảnh hưởng của đơn vị đo lường (VND vs năm tuổi)\n")
    cat("• Tăng tính công bằng giữa các biến trong quá trình NLPCA\n")
    cat("• Cải thiện độ ổn định của thuật toán tối ưu hóa\n")
    cat("• Kết quả dễ diễn giải và so sánh hơn\n")
    
    # Đánh giá tính hợp lệ (logic tương tự)
    cat("\nKIỂM TRA HỢP LỆ:\n")
    
    if(results$final_absolute_variance_pct >= 60 && results$final_absolute_variance_pct < 100) {
      cat("✓ Phương sai đạt mức hợp lý (60-99%) với dữ liệu chuẩn hóa\n")
    } else if(results$final_absolute_variance_pct >= 60) {
      cat("⚠ Phương sai chấp nhận được nhưng cao (≥60%) với dữ liệu chuẩn hóa\n")
    } else {
      cat("Phương sai dưới ngưỡng yêu cầu (<60%) với dữ liệu chuẩn hóa\n")
    }
    
    if(reduction_pct >= 15 && reduction_pct <= 70) {
      cat("✓ Giảm biến hiệu quả (15-70%)\n")
    } else if(reduction_pct > 70) {
      cat("⚠ Tỷ lệ giảm biến cao (>70%) - có thể mất thông tin quan trọng\n")
    } else {
      cat("⚠ Tỷ lệ giảm biến thấp (<15%) - chưa hiệu quả\n")
    }
    
    if(results$failed_evaluations == 0) {
      cat("✓ Hội tụ hoàn toàn (0 lần lỗi đánh giá) với dữ liệu chuẩn hóa\n")
    } else if(results$failed_evaluations < 5) {
      cat("✓ Hội tụ tốt (số lần lỗi đánh giá thấp) với dữ liệu chuẩn hóa\n")
    } else {
      cat("⚠ Có xuất hiện một số vấn đề hội tụ với dữ liệu chuẩn hóa\n")
    }
    
    cat("\nDANH SÁCH BIẾN ĐƯỢC CHỌN (SAU CHUẨN HÓA):\n")
    cat("-------------------------------------------\n")
    for(i in 1:length(results$final_variables)) {
      var <- results$final_variables[i]
      var_type <- if(var %in% numeric_vars) "[Định lượng - Đã chuẩn hóa]" else "[Định tính]"
      cat(sprintf("  %2d. %-20s %s\n", i, var, var_type))
    }
    
    cat("\nDANH SÁCH BIẾN BỊ LOẠI BỎ (theo thứ tự):\n")
    cat("----------------------------------------\n")
    if(length(results$removed_variables) > 0) {
      for(i in 1:length(results$removed_variables)) {
        var <- results$removed_variables[i]
        var_type <- if(var %in% numeric_vars) "[Định lượng]" else "[Định tính]"
        cat(sprintf("  %2d. %-20s %s\n", i, var, var_type))
      }
    } else {
      cat("  (Không có biến nào bị loại bỏ)\n")
    }
    
    # Đánh giá tổng thể (logic tương tự)
    overall_success <- (results$final_absolute_variance_pct >= 60 && 
                       results$final_absolute_variance_pct < 100 &&
                       reduction_pct >= 15 &&
                       results$failed_evaluations < 5 &&
                       results$standardized_data_used)
    
    cat("\nĐÁNH GIÁ TỔNG THỂ:\n")
    if(overall_success) {
      cat("✓ Thuật toán thực thi thành công với dữ liệu đã chuẩn hóa.\n")
      cat("✓ Kết quả tuân thủ đúng các tiêu chí lựa chọn biến với dữ liệu chuẩn hóa.\n")
      cat("✓ Logic loại biến dựa trên phương sai tuyệt đối đảm bảo đúng lý thuyết.\n")
      cat("✓ Chuẩn hóa Z-score đã loại bỏ bias do đơn vị đo lường.\n")
      cat("✓ Kết quả có ý nghĩa khoa học và thống kê cao hơn.\n")
    } else {
      cat("⚠ Thuật toán cần được kiểm tra, điều chỉnh thêm.\n")
      cat("⚠ Cần xem lại cấu hình hoặc quy trình chuẩn hóa dữ liệu.\n")
    }
    
  } else {
    cat("Thuật toán không thực hiện thành công.\n")
    cat("Lỗi gặp phải: ", results$error, "\n")
  }
  
  return(results)
}

cat("Hàm thực thi đã được cập nhật cho dữ liệu chuẩn hóa.\n\n")

# Thực thi thuật toán với dữ liệu đã chuẩn hóa
cat("Bắt đầu quy trình loại biến với dữ liệu đã chuẩn hóa Z-score...\n")

if(exists("NLPCA_RESULTS")) {
  
  # Chạy thuật toán với dữ liệu chuẩn hóa
  STANDARDIZED_RESULTS <- execute_standardized_backward_elimination(NLPCA_RESULTS)
  
  if(STANDARDIZED_RESULTS$success) {
    cat("\n✓ Quy trình loại biến với dữ liệu đã chuẩn hóa đã hoàn thành.\n")
    cat("✓ Kết quả phù hợp và tuân thủ lý thuyết với dữ liệu đã chuẩn hóa.\n")
    cat("✓ Chuẩn hóa Z-score đã cải thiện chất lượng phân tích.\n")
  } else {
    cat("\nQuy trình thực thi không thành công, cần kiểm tra lại.\n")
    cat("Lỗi gặp phải: ", STANDARDIZED_RESULTS$error, "\n")
  }
  
} else {
  cat("Không thể thực hiện do thiếu NLPCA_RESULTS.\n")
}
```
